\chapter{Methods}\label{ch:method}

The goal of this thesis is to provide a comparison between two popular approaches for real-time physics-based simulation called XPBD \cite{macklin2016}
and Projective Dynamics (PD) \cite{bouaziz2014}. PD can be extended by interpreting it through the lense of the Alternating 
Direction Method of Multipliers (ADMM) for constrained optimization \cite{overby2017} and Quasi-Newton (QN) methods for unconstrained 
optimization \cite{liu2017}, without sacrificing the efficiency of the original PD algorithm. In this chapter, we provide a detailed 
description of the simulation methods listed above together with a thorough analysis of their theoretical properties. Where applicable, 
we highlight how different simulation methods are related to each other. Additionally, we discuss if and how geometric constraints and the 
non-linear material models in Section \ref{ss:material-models} can be implemented using each of the simulation methods. XPBD and its 
predecessor PBD are covered in Section \ref{s:pbd}. PD and the QN and ADMM methods built on top of it are examined in Sections \ref{s:pd}, 
\ref{s:qn-rts} and \ref{s:admm}, respectively.

\section{Position Based Dynamics}\label{s:pbd}
As discussed in Section \ref{s:equations-of-motion}, classical approaches for dynamics simulation are force-based. Forces are accumulated and 
resulting accelerations are computed based on Newton's second law. These accelerations are then integrated over time via numerical integration. 
If successful, this strategy yields physically accurate results. However, designing integration schemes that are robust and stable,
particularly in the presence of stiff forces, is challenging. Corresponding issues often manifest themselves in the context of contact and collision 
handling. In real-time applications, physically accurate results are often not required. Thus, algorithms that yield visually
plausible simulations in a robust and stable manner are preferred. To address these needs, Müller et al.\ \cite{mueller2006} propose manipulating
positions directly on a per-constraint basis without integrating accelerations of velocities in an approach called Position Based Dynamics (PBD).
This way, collisions can simply be handled one-by-one by projecting particles to valid locations instead of by integrating accelerations from 
stiff forces, leading to improved robustness and controllability. 

The main drawback of PBD is that constraints become arbitrarily stiff when the iteration count is increased or when the time step is decreased.
Macklin et al.\ \cite{macklin2016} devise an extension of PBD called extended Position Based Dynamics (XPBD) that is derived from the implicit
integration of the equations of motion (see Eq.\ \ref{eq:equations-of-motion}) with constraint potentials based on PBD constraints. The overall structure 
of the PBD algorithms is preserved with 
minor changes to the projection of individual constraints. XPBD reduces the coupling of stiffness to iteration count and time step and relates 
constraints to corresponding, well-defined constraint forces. According to Macklin et al., XPBD and PBD are equivalent in the limit of infinite
stiffness. 

Since PBD and XPBD only differ in the way individual constraints are projected, we give a general overview over PBD-style algorithms 
in Section \ref{ss:pbd-framework}. 
The details of individual constraint projection in PBD and XPBD are covered in Section \ref{ss:pbd-constraint-projection} and 
Section \ref{ss:xpbd-constraint-projection}, respectively. A comprehensive discussion of the properties of both solvers is provided in 
Section \ref{ss:pbd-properties} and Section \ref{ss:xpbd-properties}, respectively. Lastly, we show how to simulate deformable bodies using physically-based 
material models with PBD in Section \ref{ss:pbd-deformable-bodies} and with XPBD in Section \ref{ss:xpbd-deformable-bodies}.

\subsection{Overview Over the PBD Framework}\label{ss:pbd-framework}
Both PBD and XPBD share the same algorithmic structure. Let a dynamic object be defined by a set of $m$ 
vertices with inverse masses $w_i$, positions $\vecm{q}_i$ and velocities $\vecm{v}_i$. Additionally, the motion of the object is governed by 
$r \in \mathbb{N}$ constraints of the form 

\[
C \colon \mathbb{R}^{3m} \to \mathbb{R}, \vecm{q} \mapsto C(\vecm{q})
\]

\noindent where $j$ is the constraint index. Note how constraints are defined solely in terms of particle positions. Equality and inequality constraints 
are satisfied if $C(\vecm{q}) = 0$ and $C(\vecm{q}) \geq 0$, respectively. In PBD, each constraint has an additional stiffness parameter $k_j \in [0,1]$. 
Each constraint has a cardinality $n_j \in \mathbb{N}$ and particle indices $i_1, \ldots, i_{n_j}$ of particles that actively contribute to the 
constraint value. In other words, for $l \in [1, \ldots, r]$ with $l \notin \{i_1, \ldots, i_{n_j}\}$ it is $\nabla_{\vecm{p}_l}C_j(\vecm{q}) = \vecm{0}$.

An overview over the PBD framework is given in Algorithm \ref{alg:pbd} \cite{mueller2006}. PBD and XPBD work by moving the particles according to their current 
velocities and the external forces acting on them and using the resulting positions as a starting point for constraint projection. This is achieved by 
performing symplectic Euler integration (lines 3-4). The resulting positions 
are projected onto the constraint manifolds of the constraints (line 5). Projecting a constraint means changing the positions of involved particles 
such that the constraint is satisfied and linear and angular momentum are preserved. The projected positions are used to carry out an implicit 
velocity update (line 7) and eventually passed on to the next time step (line 8) in correspondence with a Verlet integration step. Note that the only
difference between PBD and XPBD is the constraint projection in \textsc{projectConstraints} (line 5).

For general, nonlinear constraints, moving the initial estimates from the symplectic Euler integration to positions that satisfy the constraints
requires solving a nonlinear system of equations. Solving this system of equations is further complicated by the presence of inequality constraints, which
need to be added or removed depending on whether they are satisfied during the current iteration. Thus, Müller et al.\ \cite{mueller2006} opt for a 
nonlinear adaptation of the Gauss-Seidel solver in their original PBD solver. Macklin et al.\ \cite{macklin2016} adapt this approach in XPBD. 
Just like the original Gauss-Seidel algorithm, which is only suitable for linear systems of equations, 
constraints are solved independently one after another. During each constraint solve, only the particles that contribute to the current constraint are
moved while all the other particle positions remain untouched. Additionally, position updates from the projection of a constraint are immediately 
visible during the projection of the constraints following thereafter. Inequality constraints that are already satisfied are simply skipped. 
During each solver iteration, all constraints are cycled through once.

\begin{algorithm}[tb]
\caption{Position Based Dynamics Framework}\label{alg:pbd}
\begin{algorithmic}[1]
\Procedure{solvePBD}{$\vecm{q}_n$, $\vecm{v}_n$, $f_{\text{ext}}$, $h$}
\State $\vecm{q} = \vecm{q}_n, \vecm{v} = \vecm{v}_n$
\State \textbf{for} all vertices $i$ \textbf{do} $\vecm{v}_i = \vecm{v}_i + hw_i\vecm{f}_{\text{ext}}(\vecm{x}_i)$
\State \textbf{for} all vertices $i$ \textbf{do} $\vecm{p}_i = \vecm{q}_i + h\vecm{v}_i$
\State $\textsc{projectConstraints}(C_1, \ldots, C_r, \vecm{p}_1, \ldots, \vecm{p}_m)$ (\cref{alg:pbd-solver} for PBD, 
\StatexIndent[2] \cref{alg:xpbd-solver} for XPBD)
\For{all vertices $i$}
\State $\vecm{v}_i = (\vecm{p}_i - \vecm{q}_i) / h$
\State $\vecm{q}_i = \vecm{p}_i$
\EndFor
\State \textbf{return with } $\vecm{q}_{n+1} = \vecm{q}, \vecm{v}_{n+1} = \vecm{v}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Due to the fact that PBD is a geometrical method that is not derived from Newton's laws of motion (see Sec.\ \ref{ss:pbd-constraint-projection}) and that 
constraints are solved locally one after each 
other, Müller et al.\ \cite{mueller2006} take great care that projections for internal constraints, i.e.\ constraints that are independent of rigid-body 
motion, preserve linear and angular momentum. Otherwise, internal constraints may introduce ghost forces which manifest themselves in artificial 
rigid-body motion \cite{mueller2006}. Of course, non-internal constraints such as collision or 
attachment constraints may have global effects on an object. For internal constraints, it is easy to show that both momenta are automatically preserved 
if the PBD position updates are performed in the direction of the mass-weighted constraint gradient \cite{mueller2006}. Even though XPBD -- unlike 
PBD -- is in fact derived from Newton's second law, Macklin et al.\ \cite{macklin2016} arrive at position updates that are multiples of the mass-weighted 
constraint gradient as well after a couple of simplifying assumptions (see Sec.\ \ref{ss:xpbd-constraint-projection}). Thus PBD and XPBD projections 
are performed along the same direction and only differ in their scaling factors. The update formulas for a single constraint update in PBD and XPBD are 
derived in Section \ref{ss:pbd-constraint-projection} and Section \ref{ss:xpbd-constraint-projection} and the resulting algorithms for projecting all constraints 
acting on simulated bodies are given in Algorithm \ref{alg:pbd-solver} and Algorithm \ref{alg:xpbd-solver}, respectively.

\subsection{PBD Constraint Projection}\label{ss:pbd-constraint-projection}

Mueller et al.\ \cite{mueller2006} derive the projection of a single constraint in PBD as follows. Let $C$ be a constraint of cardinality $n_c$ 
acting on particles $i_1, \ldots, i_{n_c}$ with predicted positions $\vecm{p}_{i_1}, \ldots, \vecm{p}_{i_{n_c}}$. Let $k_c$ be the constraint 
stiffness. The goal is to find a position update $\Delta \vecm{p}$ such that 

\begin{equation}\label{eq:pbd-delta}
    C(\vecm{p} + \Delta \vecm{p}) = 0.
\end{equation}

\noindent To preserve linear and angular momenta, $\Delta \vecm{p}$ is required to be in the direction of the mass-weighted constraint 
gradient, or formally

\begin{equation}\label{eq:pbd-update-general}
    \Delta \vecm{p} = \lambda \matm{W} \nabla C(\vecm{p})
\end{equation}

\noindent for some $\lambda \in \mathbb{R}$ and $\matm{W} = \text{diag}(w_1, w_1, w_1, \ldots, w_m, w_m, w_m)$. 
Plugging into Equation \ref{eq:pbd-delta} and approximating by first-order Taylor expansion yields

\[
    C(\vecm{p} + \lambda \matm{W} \nabla C(\vecm{p})) \approx C(\vecm{p}) + \nabla C(\vecm{p})^T \lambda \matm{W}
    \nabla C(\vecm{p}) = 0.
\]

\noindent Solving for $\lambda$ yields

\begin{equation}\label{eq:pbd-lambda}
    \lambda = -\frac{C(\vecm{p})}{\sum_{i \in \{ i_1, \ldots, i_{n_c} \}} w_i \vert \nabla_{\vecm{p}_i}C(\vecm{p}) \vert^2 }.
\end{equation}

\noindent Plugging $\lambda$ into Equation \ref{eq:pbd-update-general} results in the final position update

\begin{equation}\label{eq:pbd-update}
    \Delta \vecm{p} = -\frac{C(\vecm{p})}{\sum_{i \in \{ i_1, \ldots, i_{n_c} \}} w_i \vert \nabla_{\vecm{p}_i}C(\vecm{p}) \vert^2 } 
    \matm{W}\nabla C(\vecm{p}).
\end{equation}

\noindent For the position of a single point $\vecm{p}_i$, this gives the update

\begin{equation}\label{eq:pbd-update-individual}
    \Delta \vecm{p}_i = -\frac{C(\vecm{p})}{\sum_{i \in \{ i_1, \ldots, i_{n_c} \}} w_i \vert \nabla_{\vecm{p}_i}C(\vecm{p}) \vert^2 } 
    w_i \nabla_{\vecm{p}_i} C(\vecm{p})
\end{equation}

Finally, the stiffness $k_c$ of the constraint needs to be taken into account. The simplest way is to simply multiply the projection update
$\Delta \vecm{p}$ by $k_c$. However, after multiple iterations of the solver, the effect of the stiffness on the update is nonlinear. Consider
a distance constraint with rest length 0 acting on predictions $\vecm{p}_{i_1}, \vecm{p}_{i_2}$ given by

\begin{equation}\label{eq:pbd-distance}
    C_{\text{dist}}(\vecm{p}) = \vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert.
\end{equation}

\noindent Then, after $n_s$ solver iterations the remaining error is going to be $\vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert (1-k)^{n_s}$. Müller et al.\ 
suggest establishing a linear relationship by multiplying corrections by $k^{\prime} = 1 - (1-k)^{1/n_s}$. This way, the error becomes
$\vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert (1-k)$ after $n_s$ solver iterations. A summary of the constraint solver is given in Algorithm 
\ref{alg:pbd-solver}.

\begin{algorithm}[H]
\caption{PBD Constraint Solver}\label{alg:pbd-solver}
\begin{algorithmic}[1]
\Procedure{projectConstraints}{$C_1, \ldots, C_r, \vecm{p}_1, \ldots, \vecm{p}_m$}
\For{all iterations $n_s$}
\ForNoDo{all constraints $C_j$ with cardinality $n_j$, } 
\StatexIndent[3] particle indices $i_1, \ldots, i_{n_j}$ \algorithmicdo
\If{$C_j$ is an inequality constraint and $C_j(\vecm{p}) \geq 0$}
\State \textbf{continue} to next constraint
\EndIf
\For{all particles $i \in \{ i_1, \ldots, i_{n_j} \}$}
\State $\Delta \vecm{p}_i = -\frac{C_j(\vecm{p})}{\sum_{i \in \{ i_1, \ldots, i_{n_j} \}} w_i \vert \nabla_{\vecm{p}_i}
C_j(\vecm{p}) \vert^2 } w_i \nabla_{\vecm{p}_i} C_j(\vecm{p})$
\State $\vecm{p}_i = \vecm{p}_i + k \Delta \vecm{p}_i$ or $\vecm{p}_i = \vecm{p}_i + (1-(1-k)^{1/n_s}) \Delta \vecm{p}_i$
\EndFor
\EndFor
\EndFor
\State \textbf{return with result } $\vecm{p}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Properties of PBD}\label{ss:pbd-properties}
Due to its simplicity and controllability, PBD is a popular choice for real-time simulations where visually plausible results are sufficient. 
In the following section, we take a closer look at the properties of the PBD solver.

\paragraph{Gauss-Seidel Solver.}
At the core of the PBD algorithm is the nonlinear Gauss-Seidel-type solver for constraint projections. Immediately making position
updates from one constraint projection visible in the following constraint projections enables faster propagation of 
constraints through the simulated body \cite{mueller2006}. However, the same property makes parallelization of the constraint projections 
in lines 8-9 of Algorithm \ref{alg:pbd-solver}
more challenging. Synchronization is required to make sure that constraints that involve the same particle do not run into race conditions. 
Alternatively, graph coloring algorithms where constraints of the same color are guaranteed to work on separate sets of particles can be employed. 
Due to the fact that constraints are handled individually, the solver is incapable of finding a 
compromise between contradicting constraints \cite{mueller2006, bouaziz2014}. In fact, oscillations can occur in over-constrained situations. 
The exact result depends on the order in which constraints are handled.

\paragraph{Relation to the Newton-Raphson Method.}
The position update due to a single constraint $C$ in Equation \ref{eq:pbd-update} is related to the Newton-Raphson method for finding roots of nonlinear 
functions $f \colon \mathbb{R} \to \mathbb{R}$ \cite{mueller2006}. There, the current guess $x_i \in \mathbb{R}$ for a root of $f$ is refined 
using the following update formula 

\begin{equation}\label{eq:newton-raphson}
    x_{i+1} = x_i - \frac{f(x_i)}{f^{\prime}(x_0)}.
\end{equation}

\noindent Indeed, applying the Newton-Raphson update to 

\begin{equation}\label{eq:newton-raphson-f}
    f \colon \mathbb{R} \to \mathbb{R}, \lambda \mapsto C(\vecm{p} + \lambda \matm{W} \nabla C(\vecm{p}))
\end{equation}

\noindent yields

\[
    \lambda_{i+1} = \lambda_i - \frac{C(\vecm{p}_i + \lambda_i \matm{W} \nabla C(\vecm{p}_i))}
    {\nabla C(\vecm{p}_i + \lambda_i \matm{W} \nabla C(\vecm{p}_i))^T \matm{W} \nabla C(\vecm{p}_i)}.
\]

\noindent With $\lambda_0 = 0$, the update formula for the first iteration is 

\[
    \lambda_1 = -\frac{C(\vecm{p}_0)}{\nabla C(\vecm{p}_0)^T \matm{W} \nabla C(\vecm{p}_0)}.
\]

\noindent Here, subscripts denote the current iteration instead of the particle index. Note that this is exactly the same as the formula for 
$\lambda$ used in the constraint solver given in Equation \ref{eq:pbd-lambda}. Thus, a single 
constraint projection corresponds to the first iteration of the Newton-Raphson method applied to Equation \ref{eq:newton-raphson-f} with 
$\lambda_0 = 0$. It is worth pointing out that the correspondence breaks down after the first iteration of PBD constraint projections. To 
see this, consider the second PBD iteration for the constraint $C$ under the assumption that the positions of the particles that $C$ acts on 
are not altered by any other constraint projections. Then, the second constraint projection is simply

\[
    \lambda_2 = -\frac{C(\vecm{p_1})}{\nabla C(\vecm{p}_1)^T \matm{W} \nabla C(\vecm{p}_1)}.
\]

\noindent However, since the positions after the first iteration are given by $\vecm{p}_1 = \vecm{p}_0 + \lambda_1 \matm{W} \nabla C(\vecm{p_0})$, 
the corresponding Newton-Raphson update is equivalent to 

\[
    \lambda_2 = \lambda_1 - \frac{C(\vecm{p}_1)}{\nabla C(\vecm{p}_1)^T \matm{W} \nabla C(\vecm{p}_0)}.
\]

\noindent Due to these differences, it is not clear whether convergence and stability guarantees of the Newton-Raphson solver are applicable to 
PBD constraint projection.

\paragraph{Stability.}
Müller et al.\ \cite{mueller2006} claim that PBD is unconditionally stable since the projected positions $\vecm{p}_i$ computed by the constraint 
solver are physically valid in the sense that all constraints are satisfied and no extrapolation into the future takes place in lines 7-8 of
Algorithm \ref{alg:pbd}. They further state that the only source of instabilities is the constraint solver itself, which is based on the 
Newton-Raphson method. The position updates in Equation \ref{eq:pbd-update-individual} are independent of the time step and solely depend on the 
shape of the constraints. 

At this point, it is worth taking into consideration that the constraint solver does not always succeed at moving 
particles to physically valid positions as implied. As mentioned above, oscillations can occur if there are contradictory constraints. 
In particular, constraint projections that are performed towards the end might undo progress achieved by previous projections. 
Moreover, we have shown above that the correspondence between the PBD constraint projections and the Newton-Raphson method breaks down 
after the first iteration. However, since it is known that the Newton-Raphson solver may fail to converge if the initial guess $\lambda_0$
is not sufficiently close to a true root $\lambda^*$ of Equation \ref{eq:newton-raphson-f}, it is expected that the same issue can occur for 
repeated applications of PBD constraint projections. Lastly, for general nonlinear constraints, it is the shape of the constraint at the 
current configuration that matters for the stability of the position update. Whether Newton-Raphson iterations are effective or not cannot be 
answered for a function -- or in this case for a constraint -- in its entirety, but only in the proximity of specific values.

\paragraph{Dependence of Stiffness On Iterations and Time Step Size.}
The main disadvantage of PBD is the fact that the stiffness depends on the iteration count and the chosen time step \cite{mueller2006}. Again, 
we take a look at a distance constraint with rest length 0 (see Eq.\ \ref{eq:pbd-distance}). As discussed in Section 
\ref{ss:pbd-constraint-projection}, the remaining error after $n_s$ solver iterations is simply $\vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert 
(1-k)^{n_s}$. In the limit of infinite iterations

\[
    \lim_{n_s \to \infty} (\vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert (1-k)^{n_s}) = 0,
\]

\noindent meaning that the distance constraint becomes infinitely stiff, regardless of the exact value of $k_c$. If instead $k^{\prime}
= 1 - (1-k)^{1/n_s}$ is used, then the error after $n_s$ solver iterations becomes $\vert \vecm{p}_{i_1} - \vecm{p}_{i_2} \vert (1-k)$. Thus,
infinite stiffness due to large iteration counts is prevented in this setting. However, the perceived stiffness still depends on the time
step. In the limit of infinitely short time steps, the material is going to appear infinitely stiff. Generally, soft bodies can only be 
simulated by restricting the number of PBD iterations.

\paragraph{Hard Constraints.}
In the previous paragraph, we pointed out that PBD constraints become infinitely stiff in the limit of infinite iterations. Thus, hard constraints 
can trivially be implemented in PBD by increasing the iteration count. Note that this increases the perceived stiffness of all constraints acting 
on the simulated system. Thus, balancing the stiffness requirements of different constraints is a major challenge in PBD.

\paragraph{Preservation of Individual Momenta.}
While the simulated object's global linear and angular momentum are preserved, the linear momenta of individual particles are at risk of 
being washed out by the PBD constraint solver \cite{bouaziz2014}. This is because even though the structure of the position updates 
preserves global momentum, there is no punishment for moving individual particles away from their inertial positions. Generally, the 
penalty for moving particles away from their inertial positions should increase with growing particle masses. However, in the PBD
position update in Equation \ref{eq:pbd-update-individual} it is only the ratio of the particle masses that matters. This can be seen by multiplying
all inverse masses $w_i$ in Equation \ref{eq:pbd-update-individual} with a constant factor $a \in \mathbb{R}^+$:

\begin{equation}\label{eq:pbd-masses}
    \begin{split}
        \Delta \vecm{p}_i 
        &= -\frac{C(\vecm{p})}{\sum_{j \in \{ i_1, \ldots, i_{n_c} \}} aw_j \vert \nabla_{\vecm{p}_j}C(\vecm{p}) \vert^2 } aw_i \nabla_{\vecm{p}_i} C(\vecm{p}) \\
        &= -\frac{C(\vecm{p})}{\sum_{j \in \{ i_1, \ldots, i_{n_c} \}} \frac{aw_j}{aw_i} \vert \nabla_{\vecm{p}_j}C(\vecm{p}) \vert^2 } \nabla_{\vecm{p}_i} C(\vecm{p}) \\
        &= -\frac{C(\vecm{p})}{\sum_{j \in \{ i_1, \ldots, i_{n_c} \}} \frac{w_j}{w_i} \vert \nabla_{\vecm{p}_j}C(\vecm{p}) \vert^2 } \nabla_{\vecm{p}_i} C(\vecm{p})
    \end{split}
\end{equation}

\noindent Note that the factor $a$ gets cancelled out, meaning that increasing or decreasing the weights of all particles in the simulation by a constant
factor does not affect position updates. Washing out of individual momenta also becomes evident in the limit of infinite iterations while multiplying 
with the stiffness $k_c$ directly, or in the limit of infinitely short time steps. In both cases, the simulated material will appear infinitely stiff, 
meaning that momenta of individual particles are not necessarily preserved.

\subsection{Simulating Deformable Bodies Using PBD}\label{ss:pbd-deformable-bodies}
As discussed in Section \ref{ss:pbd-constraint-projection}, the PBD constraint projection is designed to solve geometric constraints. However, Bender et al.\ 
\cite{bender2014} simulate continuous elastic materials by deriving constraints from physical material models. Let $\Psi$ be the energy density of the 
material model and let $E_e$, $\matm{F}_e$ and $V_e$ be the elastic energy, deformation gradient and volume of a tetrahedral element $e$, respectively
(see Sec.\ \ref{ss:deformable-bodies}). Then, the authors introduce a constraint $C_e$ with $C_e(\vecm{q}) = E_e(\vecm{q}) = V_e\Psi(\matm{F}_e)$ for each 
tetrahedral element, with small adjustments to achieve robust inversion handling. In short, the tetrahedral energies are simply repurposed as geometric 
constraints. Note that this approach for simulating 
deformable bodies comes with the disadvantages inherent to PBD (see Sec.\ \ref{ss:pbd-properties}). Most notably, the perceived stiffness of the material is 
dependent on the time step size and the number of iterations. In the limit of infinite iterations and zero time step size, the simulated body will 
appear rigid. As a result, the simulations are not physically accurate, even though the constraints are derived directly from the energy density $\Psi$. 
Still, complex physical phenomena such as lateral contraction can be simulated with PBD using this setup \cite{bender2014}.

\subsection{XPBD Constraint Projection}\label{ss:xpbd-constraint-projection}
The derivation of XPBD \cite{macklin2016} starts from the position-level implicit Euler update formula for the equations of motion 
(see Eq.\ \ref{eq:implicit-positional-detailed}), which is restated here again for the sake of convenience

\begin{equation}\label{eq:implicit-positional-detailed-2}
    \matm{M}(\vecm{q}_{n+1} - \vecm{q}_n - h\vecm{v}_n) = h^2\left(\vecm{f_\text{ext}} - \sum_j \nabla \psi_j(\vecm{q}_{n+1})\right).
\end{equation}

\noindent Let $m$ be the number of particles in the simulated body and $r$ be the number of conservative potentials $\psi_j$ with $j \in [1, r]$.
In the context of XPBD, $\vecm{q}, \vecm{v}, \vecm{f}_{\text{ext}} \in \mathbb{R}^{3m}$ and $\psi_j \colon \mathbb{R}^{3m} \to \mathbb{R}$. Simple 
manipulation of Equation \ref{eq:implicit-positional-detailed-2} yields

\begin{equation}\label{eq:implicit-positional-detailed-3}
    \matm{M} \left( \vecm{q}_{n+1} - \tilde{\vecm{q}} \right) = - h^2 \sum_j \nabla \psi_j(\vecm{q}_{n+1}),
\end{equation}

\noindent where $\tilde{\vecm{q}} = \vecm{q}_n + h\vecm{v}_n + h^2 \matm{M}^{-1} \vecm{f}_{\text{ext}}$ is the predicted, inertial position. XPBD builds on
top of the compliant constraint formulation discussed in Section \ref{ss:compliant-constraints}. In the compliant constraint framework, each $\psi_j$ 
can be written in terms of some positional constraint function $C_j$ 

\begin{equation}\label{eq:xpbd-potential-j}
    \psi_j(\vecm{q}) = \frac{1}{2} \alpha^{-1}_j C_j(\vecm{q})^2,
\end{equation}

\noindent where $\alpha_j$ is the inverse stiffness of the constraint. If the constraint functions are grouped into a vector-valued function
$\vecm{C}$ with $\vecm{C}(\vecm{q}) = [C_1(\vecm{q}), \ldots, C_r(\vecm{q})]^T$ and the inverse stiffnesses are aggregated into the diagonal matrix
$\matm{\alpha} = \text{diag}(\alpha_1, \ldots, \alpha_r)$, then

\begin{equation}\label{eq:xpbd-potential}
    \psi(\vecm{q}) \coloneqq \sum_j \psi_j(\vecm{q}) = \frac{1}{2}\vecm{C}(\vecm{q})^T \matm{\alpha}^{-1} \vecm{C}(\vecm{q}).
\end{equation}

\noindent where $\psi$ is the combined internal energy potential. The force from the internal potential is given by

\begin{equation}\label{eq:xpbd-internal-force}
    \vecm{f}_{\text{int}} = -\nabla \psi(\vecm{q}) = -\nabla \vecm{C}(\vecm{q})^T \matm{\alpha}^{-1} \vecm{C}(\vecm{q}).
\end{equation}

\noindent Plugging the internal force $\vecm{f}_{\text{int}}$ into Equation \ref{eq:implicit-positional-detailed-3} and pulling $h^2$ into the 
compliance matrix $\matm{\alpha}$ results in

\[
    \matm{M}(\vecm{q}_{n+1} - \tilde{\vecm{q}}) = - \nabla C(\vecm{q}_{n+1})^T \tilde{\matm{\alpha}}^{-1} \vecm{C}(\vecm{q}_{n+1}),
\]

\noindent where $\tilde{\matm{\alpha}} = \frac{\matm{\alpha}}{h^2}$. Now, the internal force $\vecm{f}_{\text{int}}$ is split into a directional 
and a scalar component by introducing the Lagrange multiplier

\begin{equation}\label{eq:lagrange-multiplier}
    \vecm{\lambda} = -\tilde{\matm{\alpha}}^{-1}\vecm{C}(\vecm{q}).
\end{equation}

\noindent This leads to the following nonlinear system of equations in terms of $\vecm{q}_{n+1}$ and $\vecm{\lambda}_{n+1}$:

\begin{align}
    \matm{M}(\vecm{q}_{n+1} - \tilde{\vecm{q}}) - \nabla (\vecm{q}_{n+1})^T \vecm{\lambda}_{n+1} &= \vecm{0} \label{eq:xpbd-g} \\
    \vecm{C}(\vecm{q}_{n+1}) + \tilde{\matm{\alpha}}\vecm{\lambda}_{n+1} &= \vecm{0} \label{eq:xpbd-h}.
\end{align}

\noindent The left-hand side of Equation \ref{eq:xpbd-g} and Equation \ref{eq:xpbd-h} are referred to as $\vecm{g}$ and $\vecm{h}$, respectively. The nonlinear 
system of equations is solved using a fixed-point iteration based on Newton's method. We replace the index $(n+1)$ indicating the current
time step by the index of the current guess in the fixed-point iteration indicated by $(i+1)$ for the sake of clarity. During each iteration, 
guesses $\vecm{q}_i, \vecm{\lambda}_i$ for a solution of the nonlinear system are improved by updates $\Delta \vecm{q}, \Delta \vecm{\lambda}$ to 
yield new iterates $\vecm{q}_{i+1}, \vecm{\lambda}_{i+1}$. The updates are determined by solving the following linear system of equations, 
which arises from the linearization of Equation \ref{eq:xpbd-g} and Equation \ref{eq:xpbd-h}:

\begin{equation}\label{eq:xpbd-lse}
    \begin{pmatrix}
        \matm{K} & -\nabla\vecm{C}^T(\vecm{q}_i)\\
        \nabla\vecm{C}(\vecm{q}_i) & \tilde{\matm{\alpha}}
    \end{pmatrix}
    \begin{pmatrix}
        \Delta \vecm{q} \\
        \Delta \vecm{\lambda}
    \end{pmatrix}
    = -
    \begin{pmatrix}
    \vecm{g}(\vecm{q}_i, \vecm{\lambda}_i) \\
    \vecm{h}(\vecm{q}_i, \vecm{\lambda}_i)
    \end{pmatrix},
\end{equation}

\noindent Here, $\matm{K}$ is partial derivative of $\vecm{g}$ with respect to $\vecm{q}$ at $\vecm{q}_i$ given by

\begin{equation}\label{eq:xpbd-K}
    \matm{K} = \frac{\partial \vecm{g}}{\partial \vecm{q}}(\vecm{q}_i) = \matm{M} - \frac{\partial \nabla \vecm{C}(\vecm{q_i})^T \vecm{\lambda_i}}{\partial \vecm{q}}.
\end{equation}

\noindent Note how the second term on the right side corresponds to the geometric stiffness $d\vecm{f}(\vecm{q})/d\vecm{q}$ 
\cite{tournier2015}. We refer to the system matrix of Equation \ref{eq:xpbd-lse} as $\matm{H}$. At this point, two simplifying assumptions 
are made.

\paragraph{Assumption 1.} Computing the geometric stiffness in $\matm{K}$ requires evaluating second derivatives of the constraint functions
$C_j$. This is expensive and error-prone. To avoid the challenges of computing second derivatives and to re-establish a connection 
to PBD (see Sec.\ \ref{ss:pbd-constraint-projection}), Macklin et al.\ \cite{macklin2016} drop the geometric stiffness by approximating 

\begin{equation}\label{eq:xpbd-assumption-1}
    \matm{K} \approx \matm{M}. 
\end{equation}

\noindent According to the authors, this simplification does not affect the solution that the fixed-point iteration converges to. However, 
altering the system matrix decreases the convergence rate akin to a Quasi-Newton method for solving nonlinear systems of equations.

\paragraph{Assumption 2.} Macklin et al.\ \cite{macklin2016} further assume that 

\begin{equation}\label{eq:xpbd-assumption-2}
    \vecm{g}(\vecm{q}_i, \vecm{\lambda}_i) = \vecm{0}. 
\end{equation}

\noindent If initial guesses
$\vecm{q}_0 = \tilde{\vecm{q}}$ and $\vecm{\lambda}_0 = \vecm{0}$ are used, plugging into Equation \ref{eq:xpbd-g} shows that this assumption is trivially
satisfied during the first iteration. To understand the justification for further iterations, it is helpful to take a look at the simplified
version of Equation \ref{eq:xpbd-lse} with both assumptions in place:

\begin{equation}\label{eq:xpbd-simplified-lse}
    \begin{pmatrix}
        \matm{M} & -\nabla\vecm{C}^T(\vecm{q}_i) \\
        \nabla \vecm{C}(\vecm{q}_i) & \tilde{\matm{\alpha}}
    \end{pmatrix}
    \begin{pmatrix}
        \Delta \vecm{p} \\
        \Delta \vecm{\lambda}
    \end{pmatrix}
    = -
    \begin{pmatrix}
    \vecm{0} \\
    \vecm{h}(\vecm{q}_i, \vecm{\lambda}_i)
    \end{pmatrix}.
\end{equation}

\noindent We refer to the system matrix as $\matm{H}_{\text{simp}}$. After the first iteration, the upper row of Equation \ref{eq:xpbd-simplified-lse} 
is satisfied. Thus, after the first iteration with $\vecm{q}_0 = \tilde{\vecm{q}}$ and $\vecm{\lambda}_0 = \vecm{0}$, it is

\begin{equation}
\begin{split}
    \vecm{0} = \matm{M}\Delta\vecm{q} - \nabla \vecm{C}(\vecm{q}_0)^T \Delta \vecm{\lambda} &= \matm{M}(\vecm{q}_1 - \vecm{q}_0) - \nabla \vecm{C}(\vecm{q}_0)^T 
    (\vecm{\lambda}_1 - \vecm{\lambda}_0) \\
    &= \matm{M}(\vecm{q}_1 - \tilde{\vecm{q}}) - \nabla \vecm{C}(\vecm{q}_0)^T \vecm{\lambda}_1. \label{eq:xpbd-assumption-2-detail}
\end{split}
\end{equation}

\noindent Using Equation \ref{eq:xpbd-assumption-2-detail}, $\vecm{g}(\vecm{q}_1, \vecm{\lambda}_1)$ can be rewritten as

\begin{equation}
\begin{split}
    &\vecm{g}(\vecm{q}_1, \vecm{\lambda}_1) = \vecm{g}(\vecm{q}_1, \vecm{\lambda}_1) - \vecm{0} \\
    &= \matm{M}(\vecm{q}_1 - \tilde{\vecm{q}}) - \nabla \vecm{C}(\vecm{q}_1)^T \vecm{\lambda}_1 - \vecm{0} \\
    &= \matm{M}(\vecm{q}_1 - \tilde{\vecm{q}}) - \nabla \vecm{C}(\vecm{q}_1)^T \vecm{\lambda}_1 
    - \matm{M}(\vecm{q}_1 - \tilde{\vecm{q}}) - \nabla \vecm{C}(\vecm{q}_0)^T \vecm{\lambda}_1 \\
    &= \nabla \vecm{C}(\vecm{q}_0)^T \vecm{\lambda}_1 - \nabla \vecm{C}(\vecm{q}_1)^T \vecm{\lambda}_1 \\
    &= (\nabla \vecm{C}(\vecm{q}_0)^T - \nabla \vecm{C}(\vecm{q}_1)^T) \vecm{\lambda}_1
\end{split}
\end{equation}

\noindent Note that if $\nabla \vecm{C}(\vecm{q}_0)^T = \nabla \vecm{C}(\vecm{q}_1)^T$, then 
$\vecm{g}(\vecm{q}_1, \vecm{\lambda}_1) = \vecm{0}$. Thus, Macklin et al.\ \cite{macklin2016} argue that 
$\vecm{g}(\vecm{q}_1, \vecm{\lambda}_1) \approx \vecm{0}$, as long as $\nabla \vecm{C}(\vecm{q})$ 
does not change too quickly.

Since the mass matrix $\matm{M}$ in the upper-left block of $\matm{H}_{\text{simp}}$ is invertible by design, it is possible
to take the Schur complement with respect to $\matm{M}$ to obtain a reduced system in terms of $\Delta \vecm{\lambda}$:

\begin{equation}\label{eq:xpbd-schur}
    (\nabla \vecm{C}(\vecm{q}_i) \matm{M}^{-1} \nabla \vecm{C}(\vecm{q}_i)^T + \tilde{\matm{\alpha}}) \Delta \vecm{\lambda} = -\vecm{C}(\vecm{q}_i) - 
    \tilde{\matm{\alpha}}\vecm{\lambda}_i
\end{equation}

\noindent The position update $\Delta \vecm{q}$ can be derived from $\Delta \vecm{\lambda}$ via the formula

\begin{equation}\label{eq:xpbd-position-update}
    \Delta \vecm{q} = \matm{W} \nabla \vecm{C}(\vecm{q}_i)^T \Delta \vecm{\lambda},
\end{equation}

\noindent where $\matm{W} = \matm{M}^{-1}$

Up until here, all constraints were handled together during each iteration. To make a connection to PBD and to return to the framework of a
nonlinear Gauss-Seidel solver Section \ref{ss:pbd-framework}, it is necessary to specify how to solve a single constraint. To that end we rewrite 
Equation \ref{eq:xpbd-schur} for a single constraint $C_j$ and get the update for its scalar Lagrange multiplier $\lambda_j$ by computing

\begin{equation}\label{eq:xpbd-lambda-j}
    \Delta \lambda_j = \frac{-C_j(\vecm{q}_i) - \tilde{\alpha}_j \lambda_{ji}}{\nabla C_j(\vecm{q}_i) \matm{W} \nabla C_j(\vecm{q}_i)^T + \tilde{\alpha}_j}.
\end{equation}

\noindent Here, $\lambda_{ji}$ is the value of the Lagrange multiplier of the $j$-th constraint after the $i$-th solver iteration. The position update for 
a single particle with index $l$ contributing to $C_j$ becomes

\begin{equation}\label{eq:xpbd-position-update-i}
    \Delta \vecm{q}_l = w_l \nabla_{\vecm{q}_l} C_j(\vecm{q}_i)^T \Delta \lambda_j.
\end{equation}

\noindent Note that $\Delta \lambda_j$ is scalar. Thus, the position update is a multiple of the mass-weighted gradient, just like in PBD 
(see Eq.\ \ref{eq:pbd-update-individual}).

\begin{algorithm}[t]
\caption{XPBD Constraint Solver}\label{alg:xpbd-solver}
\begin{algorithmic}[1]
\Procedure{projectConstraints}{$C_1, \ldots, C_r, \vecm{p}_1, \ldots, \vecm{p}_m$}
\State \textbf{for} all constraints $C_j$ \textbf{do} $\lambda_j = 0$
\For{all iterations $n_s$}
\ForNoDo{all constraints $C_j$ with cardinality $n_j$, particle indices $i_1, \ldots, i_{n_j}$,}
\StatexIndent[3] Lagrange multiplier $\lambda_j$ \algorithmicdo
\If{$C_j$ is an inequality constraint and $C_j(\vecm{p}) \geq 0$}
\State \textbf{continue} to next constraint
\EndIf
\State $\Delta \lambda_j = \frac{-C_j(\vecm{p}) - \tilde{\alpha}_j \lambda_{j}}{\nabla C_j(\vecm{p}) \matm{W} \nabla C_j(\vecm{p})^T + \tilde{\alpha}_j}$
\State $\lambda_{j} = \lambda_{j} + \Delta \lambda_j$
\For{all particles $i \in \{ i_1, \ldots, i_{n_j} \}$}
\State $\Delta \vecm{p}_i = \Delta \lambda_j w_i \nabla_{\vecm{p}_i} C_j(\vecm{p})$
\State $\vecm{p}_i = \vecm{p}_i + \Delta \vecm{p}_i$
\EndFor
\EndFor
\EndFor
\State \textbf{return with result } $\vecm{p}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

In summary, we simply compute $\Delta \lambda_j$ via Equation \ref{eq:xpbd-lambda-j} and use it to 
update $\lambda_{j+1} = \lambda_j + \Delta \lambda$ and to determine $\Delta \vecm{q}$ via Equation \ref{eq:xpbd-position-update} while solving the $j$-th constraint. 
This leads to a natural extension of the PBD algorithm, where the general structure in Algorithm \ref{alg:pbd} is preserved. The only changes occur in the 
computation of the scaling factor for the 
mass-weighted constraint gradient in \textsc{projectConstraints} in line 5. The XPBD version of \textsc{projectConstraints} is given in 
Algorithm \ref{alg:xpbd-solver}. Note that Algorithm \ref{alg:xpbd-solver} is specified in terms of the projection points $\vecm{p}$ or $\vecm{p}_i$ instead of the 
positions $\vecm{q}$ or $\vecm{q}_i$ used in Equation \ref{eq:xpbd-position-update} and Equation \ref{eq:xpbd-lambda-j} to maintain notational consistency with the
PBD solver in Algorithm \ref{alg:pbd-solver}.


\subsection{Properties of XPBD}\label{ss:xpbd-properties}
XPBD is a natural extension of PBD that addresses some of PBD's shortcoming while maintaining the simplicity of the original algorithm. Due to
the similarity of both algorithms, PBD implementations can readily be extended to XPBD at the minor cost of storing an additional variable per 
constraint. In the following, the properties of XPBD are discussed.

\paragraph{Relation between XPBD and PBD.}
The derivation of the XPBD constraint projection builds on the concept of compliant constraints developed by Servin et al.\ 
\cite{servin2006}. As discussed in Section \ref{ss:compliant-constraints}, the compliant constraint formulation often allows handling hard constraints by
setting $\matm{\alpha} = \matm{0}$. Indeed, a closer look at the PBD update formula in Equation \ref{eq:pbd-update-individual} and the XPBD update 
formulas in Equation \ref{eq:xpbd-lambda-j}, Equation \ref{eq:xpbd-position-update-i} reveals that XPBD and 
PBD are equivalent if the compliance term $\alpha_j$ of constraint $C_j$ is zero. This matches with the observation that bodies 
simulated by PBD are infinitely stiff in the limit of infinite iterations (see Sec.\ \ref{ss:pbd-properties}). Note that this correspondence between XPBD 
and PBD gets lost if the stiffnesses are not moved out of the constraints and moved into the compliances. To see this, consider the energy 
potential $\psi(\vecm{q}) = \frac{1}{2} \alpha^{-1} C(\vecm{q})^2$. If we construct alternative constraints $C^\prime$ with $C^\prime(\vecm{q}) = 
\alpha^{-1/2}C(\vecm{q})$, we can alternatively express the energy potential $\psi$ as follows

\[
    \psi(\vecm{q}) = \frac{1}{2} C^{\prime}(\vecm{q})^2 = \frac{1}{2} \alpha^{\prime -1}C^{\prime}(\vecm{q})^2,
\]

\noindent with compliance $\alpha^\prime = 1$ independent of material stiffness. In this setting, modelling infinite stiffness by setting 
$\alpha^\prime = 0$ does not work.

If $\alpha_j \neq 0$, the compliance terms in Equation \ref{eq:xpbd-lambda-j} regularize the constraint in such a way that the constraint force is 
limited and corresponds to the constraint potential \cite{macklin2016}. This addresses the issue of coupling between iteration count and stiffness in 
the original PBD algorithm (see Sec.\ \ref{ss:pbd-properties}). Since the time step is later baked into $\tilde{\alpha}_j$, coupling between time step size 
and stiffness is also removed.

\paragraph{Hard Constraints.}
As discussed above, the XPBD update equations are equivalent to the PBD update equations for constraints with zero compliance. Since PBD constraints appear 
infinitely stiff in the limit of infinite iterations, this provides an avenue for modelling hard constraints with XPBD. In contrast to PBD, increasing 
the iteration count does not cause the perceived stiffness of constraints with non-zero compliance to increase as a result of the compliant constraint 
formulation. Thus, hard constraints and soft constraints can be handled together effortlessly with XPBD.

\paragraph{Generality.}
The XPBD derivation builds on the assumption that energy potentials $\psi$ fit into the compliant constraint framework (see Eq.\ \ref{eq:xpbd-potential-j}).
Thus, implementing an energy potential $\psi$ in XPBD requires constructing a constraint $C$ such that 
$\psi(\vecm{q}) = \frac{1}{2} \alpha^{-1} C(\vecm{q})^2$. Since $\alpha \geq 0$ and $C(\vecm{q})^2 \geq 0$ for all $\vecm{q}$ irrespective of $C$, this 
can only be achieved for non-negative energy potentials 
$\psi$. Thus, XPBD is not compatible with energy potentials that can take on negative values without further modifications. However, we can take 
advantage of the fact that adding a constant offset $c \in \mathbb{R}$ to $\psi$ does not affect the resulting forces $\vecm{f} = d\psi / d\vecm{q}$ 
to construct an equivalent energy potential $\psi^\prime$ if $\psi$ is bounded from below. Let $b \in \mathbb{R}$ be a lower bound of $\psi$. Then,
an equivalent non-negative energy potential $\psi^\prime$ can be constructed by setting

\begin{equation}\label{eq:xpbd-potential-non-negative}
    \psi^\prime(\vecm{q}) = \psi(\vecm{q}) + \vert b \vert.
\end{equation}

\noindent Note that this strategy does not work if $\psi$ does not have a lower bound. For example, the Neohookean energy density from 
Equation \ref{eq:neohookean-material-model} is unbounded from below due to the volume-preserving log terms. Consequently, it is incompatible with the XPBD 
solver.

\paragraph{Simplifying Assumptions.}
As mentioned in Section \ref{ss:xpbd-constraint-projection}, assumption 1 (see Eq.\ \ref{eq:xpbd-assumption-1}) corresponds to dropping the geometric stiffness 
$\delta \nabla \vecm{C}(\vecm{q_i})^T \vecm{\lambda_i} / \delta \vecm{q}$. It is worth noting that with $\vecm{\lambda}_0 = \vecm{0}$, this assumption is 
trivially satisfied during the first iteration. According to Equation \ref{eq:lagrange-multiplier}, after a sufficient number of iterations it is $\vecm{\lambda}_i 
\approx \tilde{\matm{\alpha}} \vecm{C}(\vecm{q}_i)$, meaning that the entries of the geometric stiffness grow with increasing constraint stiffness and iteration
count. Thus, with stiffer constraints and larger iteration counts, assumption 1 becomes more aggressive. Tournier et al.\ \cite{tournier2015} state that
the iterative nature of PBD-style solvers ameliorates instabilities in the transverse direction of stiff constraints that is often observed as a result
of neglecting geometric stiffness.

Macklin et al.\ \cite{macklin2016} claim that replacing $\matm{H}$ with $\matm{H}_{\text{simp}}$ in assumption 1 can be interpreted as applying a quasi-Newton 
method -- also known as Broyden methods -- to the NLSE given by Equation \ref{eq:xpbd-g} and Equation \ref{eq:xpbd-h}, only affecting the convergence rate. While it is 
true that changing the Hessian 
matrix in Newton's method for unconstrained optimization to some positive definite approximation only changes the convergence rate under mild conditions
\cite{nocedal2006}, it is difficult to verify whether this also holds for the simplification from $\matm{H}$ to $\matm{H}_{\text{simp}}$ in the context of
solving NLSEs. There, it is often the case that quasi-Newton methods are not guaranteed to converge at all if aggressive approximations
of the system matrix are performed \cite{nocedal2006}. This is because there is no natural merit function available to help with the selection of 
step sizes along the suggested update directions. In their XPBD derivation, Macklin et al.\ \cite{macklin2016} do
point out that a line search strategy might be required to keep the fixed-point iteration based on Equation \ref{eq:xpbd-lse} robust, but it is also important
to mention that approximations of the system matrix without an appropriate line search procedure in place are potentially more aggressive.

Assumption 2 in the XPBD derivation is justified by the observation that Equation \ref{eq:xpbd-assumption-2-detail} and $\vecm{g}(\vecm{q}_1, 
\vecm{\lambda}_1)$ are the same, except that $\nabla \vecm{C}(\vecm{q}_0)^T$ is replaced by $\nabla \vecm{C}(\vecm{q}_1)^T$. Thus, Macklin et al.\ 
\cite{macklin2016} claim that Equation \ref{eq:xpbd-assumption-2-detail} is close to zero as well if $\vecm{g}(\vecm{q}_0, \vecm{\lambda}_0) = \vecm{0}$, 
which is true by definition of $\vecm{q}_0$ and $\vecm{\lambda}_0$. However, this neglects the fact that $\nabla \vecm{C}(\vecm{q}_i)$ occurs in 
a product with $\vecm{\lambda}_i$ in $\vecm{g}(\vecm{q}_i, \vecm{\lambda}_i)$. As mentioned in the discussion of assumption 1, 
$\vecm{\lambda}_i$ gets very large for stiff constraints after a 
sufficient number of iterations. Thus, even small changes to $\nabla \vecm{C}(\vecm{q}_i)$ eventually drive the value of $\vecm{g}$ away from zero
significantly. As a result, assumption 2 becomes more aggressive with stiffer constraints and larger iteration counts, just like observed
for assumption 1. Additionally, assuming that $\vecm{g}(\vecm{q}_i, \vecm{\lambda}_i) = \vecm{0}$ leads to a change of the right side between 
the LSEs in Equation \ref{eq:xpbd-lse} and Equation \ref{eq:xpbd-simplified-lse}. In contrast to the changes to the system matrix during assumption 1, 
this does affect the solution the solver converges to. Thus, due to assumption 2, the XPBD solver cannot be said to solve the original 
implicit equations of motion \cite{macklin2016}.

\paragraph{Preservation of Individual Momenta.}
When investigating the properties of PBD (see Sec.\ \ref{ss:pbd-properties}), we mentioned that multiplying all particle masses $m_i$ with a constant
positive factor $a \in \mathbb{R}^+$ does not impact the PBD update, highlighting that there is no punishment for moving individual
particles from their inertial positions in PBD. The fact that both positions and Lagrange multipliers are updated during each
iteration of the XPBD solver makes an analysis similar to Equation \ref{eq:pbd-masses} for XPBD challenging for arbitrary iterations $i$. However, it 
is simple to look at the effect of scaling all particle masses by $a$ on the position update during the first iteration, assuming that 
$\vecm{\lambda} = \vecm{0}$. The resulting update of particle $i$ is given by

\begin{equation}\label{eq:xpbd-masses}
    \Delta \vecm{q}_i 
    = -\frac{C(\vecm{q})}{\sum_{j \in \{ i_1, \ldots, i_{n_c} \}} \frac{w_j}{w_i} \vert \nabla_{\vecm{q}_j}C(\vecm{q}) \vert^2 + \tilde{\alpha}a m_i} 
    \nabla_{\vecm{q}_i} C(\vecm{q}).
\end{equation}

\noindent Note how the factor $a$ does occur in the second summand in the denominator in a way that decreases the size of the position update
if $a$ is increased. However, it appears in a product with the small $\tilde{\alpha}$. Thus, the effect of increasing all particle masses on
the position update is still rather small. It is worth pointing out that the inclusion of $a$ in the position update of the first iteration
penalizes moving particles with large masses in all directions. Ideally, moving particles towards the inertial positions $\tilde{\vecm{q}}$
should be encouraged whereas as moving particles away from $\tilde{\vecm{q}}$ should be discouraged. This directionality is only possible if
$\tilde{\vecm{q}}$ is not simply used as an initial guess for the XPBD solver, but also used explicitly in the update equations. However, setting
$\vecm{g}(\vecm{q}_i, \vecm{\lambda}_i) = \vecm{0}$ in assumption 2 removes all occurrences of the inertial positions from Equation \ref{eq:xpbd-simplified-lse}.

\paragraph{Initial Guess For $\vecm{\lambda}_0$.}
Both assumptions 1 and 2 exploit the choice $\vecm{\lambda}_0 = \vecm{0}$. It is worth mentioning that this precludes the use of the more natural
initial guess given by $\vecm{\lambda}_0 = -\tilde{\matm{\alpha}} \vecm{C}(\tilde{\vecm{q}})$. This candidate results from plugging the 
inertial positions into the
definition of the Lagrange multipliers in Equation \ref{eq:lagrange-multiplier}. Picking an initial guess that is as close as possible to the true
solution is important for keeping the linearization error during the fixed-point iteration based on Newton's method in the XPBD derivation 
small. 

\paragraph{Numerical Damping.}
While motion due to external forces is handled by the symplectic Euler integration in lines 3-4 of Algorithm \ref{alg:pbd}, the way 
elasticity is handled is derived from the implicit equations of motions (see Eq.\ \ref{eq:implicit-positional-detailed-2}). As a result 
elastic forces are subjected to numerical damping that gets more severe with growing time step sizes in XPBD (see Sec.\ \ref{ss:numerical-integration}). 
For this reason, even though baking the time step into $\tilde{\matm{\alpha}}$ in the compliant constraint formulation reduces coupling 
between time step and stiffness, the perceived stiffness of simulated materials is still time step dependent to some extent.

\paragraph{Gauss-Seidel Solver.}
Finally, it is worth pointing out that the implications of using a Gauss-Seidel-type solver discussed in the context of PBD 
(see Sec.\ \ref{ss:pbd-properties}) apply to XPBD as well.

\subsection{Simulating Deformable Bodies Using XPBD}\label{ss:xpbd-deformable-bodies}
To simulate deformable bodies using the material models presented in Section \ref{ss:material-models}, we introduce constraints $C_j$ with 
compliance $\alpha_j$ for each tetrahedral element such that the corresponding constraint energy 
potential $\psi_j$ satisfies

\begin{equation}\label{eq:xpbd-deformation-goal}
    \psi_j(\vecm{q}) = \frac{1}{2}\alpha^{-1}_jC_j(\vecm{q})^2 = V_j\Psi_{\text{mat}}(\matm{F}_j).
\end{equation}

\noindent Here, $V_j$ is the volume of the undeformed tetrahedron, $\Psi_{\text{mat}}$ is the energy density of the material 
model and $\matm{F}_j$ is the deformation gradient of the deformed tetrahedron. As discussed in Section \ref{ss:xpbd-properties}, this 
cannot be achieved for the classical Neohookean material model in Equation \ref{eq:neohookean-material-model}. In the following, we show 
how to implement the strain material model (see Eq.\ \ref{eq:strain-material-model}) and the simplified Neohookean material model 
(see Eq.\ \ref{eq:smith-neohookean-material-model}) in XPBD.

\paragraph{Strain Material Model.}
For the strain material model $\Psi_{\text{strain}}$ in Equation \ref{eq:strain-material-model}, Equation \ref{eq:xpbd-deformation-goal} is 
satisfied by setting 

\[
    \alpha_j = \frac{1}{k V_j} \, \, \, \text{and} \, \, \, C_j(\vecm{q}) = \norm{\matm{I} - \matm{\Sigma}_j}_F,
\]

\noindent where $k$ is the stiffness of the strain material model and $\matm{\Sigma}_j$ is the diagonal matrix whose entries 
are the singular values of $\matm{F}_j$. Note that $k$ only appears in the compliance $\alpha_j$, but not in of the constraint 
$C_j$. Thus, the behavior of PBD can be recovered by setting $\alpha_j = 0$.

\paragraph{Simplified Neohookean Material Model.}
For the simplified Neohookean material model in Equation \ref{eq:smith-neohookean-material-model}, constructing suitable constraints 
$C_j$ and compliances $\alpha_j$ is more complicated. This is due to the fact that $\Psi_{\text{Smith}}$ can take on 
negative values. First, we observe that $\Psi_{\text{Smith}}$ is bounded from below by rearranging terms as follows

\begin{align*}
    \Psi_{\text{Smith}}(\matm{F}) &= \frac{\mu}{2}(\text{tr}(\matm{F}^T\matm{F}) - 3) + \frac{\lambda}{2}\left(\text{det}(\matm{F}) - 
    \left(1 + \frac{\mu}{\lambda}\right)\right)^2\\
                                  &= \frac{\mu}{2}\text{tr}(\matm{F}^T\matm{F}) + \frac{\lambda}{2}\left(\text{det}(\matm{F}) - 
                                  \left(1 + \frac{\mu}{\lambda}\right)\right)^2 - \frac{3\mu}{2}.
\end{align*}

\noindent Note that the first two terms are non-negative, while the last term is negative and independent of 
$\vecm{q}$. Consequently, $-\frac{3\mu}{2}$ is a lower bound of $\Psi_{\text{Smith}}$ and the equivalent 
non-negative energy density $\Psi^\prime_{\text{Smith}}$ given by

\[
    \Psi^\prime_{\text{Smith}}(\mat{F}) = \frac{\mu}{2}\text{tr}(\matm{F}^T\matm{F})     
    + \frac{\lambda}{2}\left(\text{det}(\matm{F}) - \left(1 + \frac{\mu}{\lambda}\right)\right)^2
\]

\noindent can be constructed (see Sec.\ \ref{ss:xpbd-properties}). We refer to the first and second term of 
$\Psi^\prime_{\text{Smith}}$ as the deviatoric and hydrostatic term, respectively.

To prepare pulling out the large Lamé coefficients $\mu$ and $\lambda$ that 
control the stiffness and incompressibility of the simulated material, we factor out the larger coefficient 
$x \coloneqq \text{max}(\mu, \lambda)$ as follows

\[
    \Psi^\prime_{\text{Smith}}(\mat{F}) = \frac{x}{2} \left(\frac{\mu}{x}\text{tr}(\matm{F}^T\matm{F})     
    + \frac{\lambda}{x}\left(\text{det}(\matm{F}) - \left(1 + \frac{\mu}{\lambda}\right)\right)^2\right).
\]

\noindent Now, Equation \ref{eq:xpbd-deformation-goal} is satisfied for $\Psi_{\text{mat}} = \Psi^\prime_{\text{Smith}}$ if 
we set

\[
    \alpha_j = \frac{1}{x V_j} \, \, \, \text{and} \, \, \, C_j(\vecm{q}) = \sqrt{\frac{\mu}{x}\text{tr}(\matm{F}_j^T\matm{F}_j)     
    + \frac{\lambda}{x}\left(\text{det}(\matm{F}_j) - \left(1 + \frac{\mu}{\lambda}\right)\right)^2}.
\]

\noindent We refer to this type of constraint as the combined Neohookean constraint. Note that $x$ does not only appear in the 
compliance $\alpha_j$, but in the constraint $C_j$ as well. Thus, this formulation can only be applied directly if $\alpha_j \neq 0$. 
Otherwise, we need to adjust $C_j$ in order to maintain consistency by either computing $C^\mu_j \coloneqq \lim_{\mu\to\infty}C_j$ 
or $C^\lambda_j \coloneqq \lim_{\lambda\to\infty}C_j$. If $x = \mu$, then $C^\mu_j$ is undefined since $\lim_{\mu\to\infty}$ diverges. 
However, if $x = \lambda$, it is easy to see that $C^\lambda_j$ must be

\begin{equation}\label{eq:combined-neohookean-0-compliance}
    C^\lambda_j(\vecm{q}) = \lim_{\lambda\to\infty}  C_j(\vecm{q}) = \sqrt{(\text{det}(\matm{F}_j) - 1)^2} = \text{det}(\matm{F}_j) - 1.
\end{equation}
    

\noindent Note that $C^\lambda_j$ only enforces that the volume of its tetrahedron is preserved. Still, it does not prevent stretching the 
tetrahdron in one direction if compressing it in another direction balances out the change in volume. 

Macklin et al.\ \cite{macklin2021} suggest a different way of implementing the simplified Neohookean material model with XPBD. The trick is 
to introduce not one, but two constraints for each tetrahedral element. The authors construct separate constraints $C^D_j$ with compliance 
$\alpha^D_j$ and $C^H_j$ with compliance $\alpha^H_j$ for the deviatoric and hydrostatic terms of $\Psi^\prime_{\text{Smith}}$, respectively, 
given by 

\begin{equation}\label{eq:deviatoric-constraint}
    C^D_j(\vecm{q}) = \sqrt{\text{tr}(\matm{F}_j^T\matm{F}_j)}, \, \, \, \, \, \, \alpha^D_j = \frac{1}{\mu V_j}
\end{equation}

\noindent and 

\begin{equation}\label{eq:hydrostatic-constraint}
    C^H_j(\vecm{q}) = \text{det}(\matm{F}_j) - \left(1 + \frac{\mu}{\lambda}\right), \, \, \, \, \, \, \alpha^H_j = \frac{1}{\lambda V_j}.
\end{equation}

\noindent It is easily verified that

\[
    \psi_j(\vecm{q}) = \frac{1}{2}(\alpha^{D}_j)^{-1} C^D_j(\vecm{q})^2 + \frac{1}{2}(\alpha^{H}_j)^{-1} C^H_j(\vecm{q})^2 
    = V_j\Psi^\prime_{\text{Smith}}(\matm{F}_j).
\]

The advantage of splitting constraints becomes apparent when modelling hard incompressibility by setting $\alpha^H_j = 0$. Since $\lambda$ 
appears in both $\alpha^H_j = 0$ and $C^H_j$, it is necessary to adjust $C^H_j$ by computing $\lim_{\lambda\to\infty}C^H_j$ again. It is 

\[
    \lim_{\lambda\to\infty}C^H_j(\vecm{q}) = \text{det}(\matm{F}_j) - 1,
\]

\noindent which is the same as what was observed for combined Neohookean constraints in Equation \ref{eq:combined-neohookean-0-compliance}.
However, this time, there is an independent deviatoric constraint $C^D_j$ that additionally drives the tetrahedron towards configurations 
where the principal stretches of the deformation gradient $\matm{F}_j$ are not unevenly distributed. Note that setting $\alpha^D_j = 0$ is 
impossible again, since $\lim_{\mu\to\infty}C^H_j$ diverges.

On the other hand, constructing separate constraints for the deviatoric and hydrostatic terms also introduces problems. In 
Section \ref{ss:simplified-neohookean-material}, we discussed that the hydrostatic term needs to be corrected to ensure the rest stability of the simplified 
Neohookean material model. The 
correction is chosen such that tetrahedral elements are artificially inflated by exactly the right amount to make up for the deflation caused 
by the deviatoric term. The derivation of the correction term relies on the assumption that both the deviatoric and the hydrostatic term act 
on the same positions simultaneously \cite{smith2018}. However, due to the nature of the Gauss-Seidel-type solver used in XPBD, the constraints 
$C^D_j$ and $C^H_j$ change the tetrahedral positions after each other. Thus, elements are inflated and deflated sequentially during each solver 
iteration. This way, rest stability cannot be ensured. The issue is particularly severe for simulations with large Lamé coefficient $\mu$, 
since this increases the degree to which both $C^D_j$ and $C^H_j$ deflate and inflate the elements, respectively. Similarly, we expect 
larger time steps to make matters worse.

\section{Projective Dynamics}\label{s:pd}

In the approaches to physical simulations via implicit time integration that we have encountered so far, a new linear system needs to be solved
at every time step. If the linear system is solved directly, this can quickly become prohibitively expensive for large simulations since 
a new matrix factorization needs to be computed every time a new system needs to be solved. In XPBD, this issue is dealt with by using an iterative 
solver. In Projective Dynamics (PD), Bouaziz et al.\ \cite{bouaziz2014} instead restrict energy potentials to a specific structure 
which allows for efficient implicit time integration via alternating steps of local and global optimization \cite{bouaziz2014}. The local 
optimization steps are comprised of per-constraint projections of particle positions onto constraint manifolds. The global optimization 
step combines the results from the individual 
local projection steps while taking into consideration global effects including inertia and external forces. This is achieved by solving a 
linear system of equations whose system matrix is constant across time steps. Since the local steps can be carried out in parallel and the 
factorization for the system matrix of the global step can be precomputed and reused, physical simulations that are restricted to energy 
potentials from the PD framework can be solved efficiently. In Section \ref{ss:pd-overview}, an overview over the PD solver is provided. There, 
we focus on introducing the broad structure of PD energy potentials and the way it benefits the convergence properties of the solver. In 
Section \ref{ss:pd-potentials}, an exact formulation of PD energy potentials is provided. This formulation is used in Section \ref{ss:pd-solver} 
to fill in the gaps in the overview given in Section \ref{ss:pd-overview} and to give a complete description of the PD solver. The 
properties of the PD solver are discussed in Section \ref{ss:pd-properties}. We show how to simulate deformable bodies using the strain 
material model (see Eq.\ \ref{eq:strain-material-model}) in Section \ref{ss:pd-deformable-bodies}. Lastly, we make a connection between the PD 
solver and Quasi-Newton methods for unconstrained optimization in Section \ref{ss:pd-quasi-newton}.

\subsection{Overview Over PD}\label{ss:pd-overview}
Projective Dynamics starts from the variational form of implicit Euler integration of the equations of motion (see Eq.\ \ref{eq:variational-implicit}),
which is restated here again for the sake of convenience

\begin{equation}\label{eq:pd-variational-implicit}
    \min_{\matm{q}_{n+1}} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\matm{q}_{n+1} - \tilde{\matm{q}})}^2_F + \sum_j \psi_j(\matm{q}_{n+1}).
\end{equation}

\noindent Recall that in the context of PD, the particle positions are stored in matrices, i.e.\ $\matm{q}_{n+1}, \tilde{\matm{q}} \in 
\mathbb{R}^{m \times 3}$ and $\matm{M} \in \mathbb{R}^{m \times m}$, where $m \in \mathbb{N}$ is the number of particles in the simulated
body. In this section, we simply write $\matm{q}$ instead of $\matm{q}_{n+1}$ to improve legibility. The solution of this unconstrained 
optimization problem corresponds to the particle positions of the simulated body at the next time
step (see Sec.\ \ref{ss:numerical-integration}) and can be determined using general purpose algorithms for unconstrained optimization 
such as Newton's method. Naive application of line search methods can often lead to unfeasibly high runtimes for real-time applications. 
The core idea of PD
is to speed up the optimization by restricting the energy potentials $\psi_j$ to a structure that allows for efficient minimization.
This allows substituting the line search methods -- which are designed for the minimization of general nonlinear objective functions 
-- with a specialized solver that exploits the structure of PD energy potentials. 

Consider the restriction of energy potentials $\psi_j$ to quadratic functions of $\matm{q}$. Then, as the sum of the inertial term 
(always quadratic) and the constraint energy (quadratic by assumption) the entire objective function of Equation \ref{eq:pd-variational-implicit} 
is quadratic in $\matm{q}$. Thus, the solution to the minimization problem can be found in a single linear solve \cite{nocedal2006}. 
While this choice
of energy potentials does enable efficient implicit Euler integration of the equations of motion, it comes at the sacrifice of generality.
In particular, quadratic energy potentials always yield linear forces. Linear forces generally do not suffice to model realistic
elastic behavior \cite{wang2011}. Now, the challenge lies in enhancing quadratic energy potentials to allow expressing some degree
of nonlinearity while still keeping the minimization efficient. 

In PD, Bouaziz et al.\ \cite{bouaziz2014} achieve this by introducing energy potentials that roughly measure the square of the
distance of the current particle positions to their projections onto some -- potentially nonlinear -- constraint manifold $\cman{C}_j$. 
In this setting, the energy potential $\psi_j$ is defined entirely by the squared distance function $d_j$ and the corresponding constraint 
manifold $\cman{C}_j$. By picking an appropriate distance measure, the squared distance $d_j$ can be made quadratic in $\matm{q}$ 
(see Sec.\ \ref{ss:pd-potentials}). Since the constraint manifold $\cman{C}_j$ is potentially nonlinear, the energy potential 
$\psi_j$ can produce nonlinear forces as well. 

Assume that the constraint has cardinality 
$n_j \in \mathbb{N}$ and acts on the particles with indices $i_1, \ldots, i_{n_j}$ and let $\matm{q}_j \in \mathbb{R}^{n_j \times 3}$ be

\[
    \matm{q}_j = \begin{pmatrix}
        \matm{q}_{i_1}^T \\
        \ldots \\
        \matm{q}_{i_{n_j}}^T
    \end{pmatrix}.
\]

\noindent Then, Bouaziz et al.\ \cite{bouaziz2014} introduce auxiliary variables $\matm{p}_j \in \mathbb{R}^{n_j \times 3}$ per constraint 
and define the energy potential in terms of the function $W_j$ given by 

\begin{equation}\label{eq:pd-Wj}
    W_j(\matm{q}_j, \matm{p}_j) = d_j(\matm{q}_j, \matm{p}_j) + \delta_{\cman{C}_j}(\matm{p}_j).
\end{equation}

\noindent Here, $\delta_{\cman{C}_j}$ is an indicator function with 

\begin{equation}\label{eq:indicator-function}
\delta_{\cman{C}_j}(\matm{p}_j)= 
\begin{cases}
0,& \text{if } \matm{p}_j \text{ lies on the constraint manifold } \cman{C}_j \\
\infty,& \text{otherwise.}
\end{cases}
\end{equation}


\noindent Consider $\matm{p}_{\matm{q}_j} \coloneqq \argmin{\matm{p}_j} W(\matm{q}_j, \matm{p}_j)$ while keeping $\matm{q}_j$ 
fixed. Then obviously 
$\delta_{\cman{C}_j}(\matm{p_{\matm{q}_j}}) = 0$, meaning that $\matm{p_{\matm{q}_j}}$ lies on $\cman{C}_j$. Additionally, 
for all other
$\matm{p}^* \in \mathbb{R}^{n_j \times 3}$ with $\delta_{\cman{C}_j}(\matm{p}^*) = 0$ it is $d_j(\matm{q}_j, \matm{p}_{\matm{q}_j}) 
\leq
d_j(\matm{q}_j, \matm{p}^*)$. Together, $\matm{p}_{\matm{q}_j}$ is the configuration on the constraint manifold that is the closest to 
the configuration specified by positions $\matm{q}_j$ in terms of $d_j$. Thus, $\matm{p}_{\matm{q}_j}$ can be 
interpreted as the projection of $\matm{q}_j$ onto the constraint manifold $\cman{C}_j$. Note that $d_j$ itself is not a distance 
function, but $\sqrt{d_j}$ is (see Sec.\ \ref{ss:pd-potentials}). Still, we follow the example of Bouaziz et al.\ \cite{bouaziz2014} 
and define closeness between pairs of configurations in terms of $d_j$ instead of $\sqrt{d_j}$. This is justified since for two pairs
of configurations $\matm{q}, \matm{p}$ and $\matm{q}^\prime, \matm{p}^\prime$ it is 

\[
    d_j(\matm{q}, \matm{p}) < d_j(\matm{q}^\prime, \matm{p}^\prime) \iff \sqrt{d_j}(\matm{q}, \matm{p}) < \sqrt{d_j}(\matm{q}^\prime, 
    \matm{p}^\prime).
\]

Bouaziz et al.\ \cite{bouaziz2014} define the energy potential $\psi_j$ as 

\begin{equation}\label{eq:pd-min-W}
    \psi_j(\matm{q}) = \min_{\matm{p}_j} W(\matm{S}_j\matm{q}, \matm{p}_j) 
    = \min_{\matm{p}_j} d_j(\matm{q}_j, \matm{p}_j) + \delta_{\cman{C}_j}(\matm{p}_j)
    = d_j(\matm{q}_j, \matm{p}_{\matm{q}_j}),
\end{equation}

\noindent where $\matm{S}_j$ is the matrix that maps $\matm{q}$ to $\matm{q}_j$. This is exactly the squared distance as measured by 
$d_j$ between configuration $\matm{q}_j$ and its projection $\matm{p}_{\matm{q}_j}$ onto the constraint manifold $\cman{C}_j$. 
Plugging the energy potentials into the variational form of implicit Euler integration (see Eq.\ \ref{eq:pd-variational-implicit}) yields

\begin{equation}\label{eq:pd-variational-overview}
    \min_{\matm{q}, \matm{p}_j} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\matm{q} - \tilde{\matm{q}})}^2_F + 
    \sum_j d_j(\matm{S}_j \matm{q}, \matm{p}_j) + \delta_{\cman{C}_j}(\matm{p}_j).
\end{equation}

\noindent Here, with abuse of notation, $\vecm{p}_j$ denotes either the auxiliary variable of the $j$-th constraint or 
the family of auxiliary variables $(\vecm{p}_j)_{j \in \mathcal{J}}$, where $\mathcal{J}$ is the index set of the 
constraints. Note that if the projections $\matm{p}_{\matm{q}_j}$ are known in advance and kept fixed for all constraint 
manifolds $C_j$, then the minimization problem becomes

\begin{equation}\label{eq:pd-global-d}
    \min_\matm{q} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\matm{q} - \tilde{\matm{q}})}^2_F + \sum_j d_j(\matm{S}_j
    \matm{q}, \matm{p}_{\matm{q}_j}).
\end{equation}

\noindent In this case, the objective function is a quadratic function of $\matm{q}$ again. As a result, it can be optimized in a single 
linear step. 

These insights suggest a local/global structure for the PD solver. In the local step, the projection points $\matm{p}_{\matm{q}_j}$
are computed for each constraint. Finding the projection points $\matm{p}_{\matm{q}_j}$ corresponds to minimizing 
Equation \ref{eq:pd-variational-overview} over the projection variables $\matm{p}_j$ while keeping the positions $\matm{q}$ fixed, yielding
the following optimization problem

\begin{equation}\label{eq:pd-local-d}
    \min_{\matm{p}_j} \sum_j d_j(\matm{S}_j \matm{q}, \matm{p}_j) + \delta_{\cman{C}_j}(\matm{p}_j).
\end{equation}

\noindent Since each constraint has its own auxiliary projection variables, this minimization can be carried out independently 
for each constraint. In the global step, Equation \ref{eq:pd-global-d} is minimized, which is equivalent to minimizing 
Equation \ref{eq:pd-variational-overview} over the positions $\matm{q}$ while keeping the projection variables $\matm{p}_j$ 
fixed. Local and global steps are repeated for a fixed number of iterations during each time step. Finally, the 
resulting positions are used as the positions of the next time step.

\subsection{PD Energy Potentials}\label{ss:pd-potentials}
To arrive at a working implementation of the PD solver outlined in Section \ref{ss:pd-overview}, a squared distance function 
$d_j$ and a projection operator onto some constraint manifold $\cman{C}_j$ need to be provided for each energy potential $\psi_j$. 
While the projection operators vary significantly between different types of constraints, Bouaziz et al.\ \cite{bouaziz2014} 
always use squared distance functions $d_j$ of the form

\begin{equation}\label{eq:pd-distance}
    d_j(\matm{q}_j, \matm{p}_j) = \frac{w_j}{2} \norm{\matm{A}_j\matm{q}_j - \matm{B}_j\matm{p}_j}^2_F,
\end{equation}

\noindent where $\matm{A}_j, \matm{B}_j$ are matrices of appropriate dimensions and $w_j \in \mathbb{R}$ is the weight assigned to the
$j$-th constraint. Plugging the definition of $d_j$ in Equation \ref{eq:pd-distance} into the PD energy potentials in Equation \ref{eq:pd-min-W} 
yields

\begin{equation}\label{eq:pd-potentials}
    \psi_j(\matm{q}) = \min_{\matm{p}_j} \frac{w_j}{2} \norm{\matm{A}_j\matm{q}_j - \matm{B}_j\matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j).
\end{equation}

Note that $\sqrt{d_j}$ is the distance function that is induced by a Frobenius norm with weights $\matm{A}_j, 
\matm{B}_j$ in the standard manner. Formally, $d_j$ itself cannot be considered a distance function since it violates the absolute
homogeneity property, i.e. 

\[
    d_j(a\matm{q}_j, a\matm{p}_j) = a^2d_j(\matm{q}_j, \matm{p}_j) \neq \vert a \vert d_j(\matm{q}_j, \matm{p}_j) \text{ for } 
    a \in \mathbb{R}. 
\]


\subsection{Projective Implicit Euler Solver}\label{ss:pd-solver}
The PD solver can be derived by plugging the squared distance functions from Equation \ref{eq:pd-distance} into the variational 
form of implicit
Euler integration for PD energy potentials (see Eq.\ \ref{eq:pd-variational-overview}). This yields the optimization problem given by 

\begin{equation}\label{eq:pd-variational}
    \min_{\matm{q}, \matm{p}_j} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\matm{q} - \tilde{\matm{q}})}^2_F + 
    \sum_j \frac{w_j}{2} \norm{\matm{A}_j\matm{S}_j\matm{q} - \matm{B}_j\matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j)
\end{equation}

\noindent To make the relation between the projection variables $\matm{p}_j$ and the relevant particle positions 
$\matm{q}_j$ more apparent, we required $\text{dim}(\matm{S}_j\matm{q}) = \text{dim}(\matm{p}_j)$ so far. However, the 
notation can be simplified by introducing the alternative constraint manifold $\cman{C}^\prime_j = \{ \matm{B}_j\matm{p} 
\mid \matm{p} \in \cman{C}_j \}$. Of course, this requires changing the shape of the projection variables. If 
$\matm{B}_j \in \mathbb{R}^{r \times n_j}$, then $\matm{p}_j \in \mathbb{R}^{r \times 3}$. If further $\matm{A}_j\matm{S}_j$ 
are combined into a single matrix $\matm{G}_j$, an equivalent optimization problem given by 

\begin{equation}\label{eq:pd-minimization}
    \min_{\matm{q}, \matm{p}_j} \tilde{g}(\matm{q}, \matm{p}_j) = 
    \min_{\matm{q}, \matm{p}_j} \frac{1}{2h^2} \norm{\matm{M}^{1/2}(\matm{q} - \tilde{\matm{q}})}^2_F + \sum_j \frac{w_j}{2} \norm{\matm{G}_j\matm{q}
    - \matm{p}_j}^2_F + \delta_{\cman{C}^\prime_j}(\matm{p}_j)
\end{equation}

\noindent can be derived. From now on, we simply write $\cman{C}_j$ instead of $\cman{C}^\prime_j$.

As discussed in Section \ref{ss:pd-overview}, the local step consists of minimizing the objective function 
Equation \ref{eq:pd-minimization} over the auxiliary variables $\matm{p}_j$ while keeping the positions $\matm{q}$ fixed. For each 
energy potential, we solve the following minimization problem

\begin{equation}\label{eq:pd-local-minimization}
    \min_{\matm{p}_j} \tilde{g}(\matm{q}, \matm{p}_j) =
    \min_{\matm{p}_j} \frac{w_j}{2}\norm{\matm{G}_j\matm{q} - \matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j).
\end{equation}

\noindent In the global step, the minimization problem Equation \ref{eq:pd-minimization} is optimized over the positions $\matm{q}$ 
while keeping the auxiliary variables $\matm{p}_j$ fixed. The optimization problem for the global solve is 
given by

\begin{equation}\label{eq:pd-global-minimization}
    \min_{\matm{q}} \tilde{g}(\matm{q}, \matm{p}_j) =
    \min_{\matm{q}} \frac{1}{2h^2} \norm{\matm{M}^{1/2}(\matm{q} - \tilde{\matm{q}})}^2_F + \sum_j \frac{w_j}{2} \norm{\matm{G}_j\matm{q} - \matm{p_j}}^2_F.
\end{equation}

\noindent The gradient of the objective function with respect to the positions $\nabla_{\matm{q}} 
\tilde{g}(\matm{q}, \matm{p}_j)$ is given by 

\begin{equation}\label{eq:pd-gradient-q}
    \nabla_{\matm{q}}\tilde{g}(\matm{q}, \matm{p}_j) = \frac{1}{h^2}\matm{M}(\matm{q} - \tilde{\matm{q}}) + \sum_j w_j \matm{G}^T_j \matm{G}_j \matm{q}
    - \sum_j w_j \matm{G}^T_j \matm{p}_j.
\end{equation}

\noindent By design of the PD energy potentials, the objective function of the global optimization problem is quadratic in the positions 
$\matm{q}$. Consequently, the minimization can be carried out in a single step by picking $\matm{q}$ such that the first-order optimality 
conditions are satisfied \cite{nocedal2006}. This leads to the following system of equations

\begin{equation}\label{eq:pd-global-system}
    (\frac{1}{h^2}\matm{M} + \sum_j w_j \matm{G}_j^T \matm{G}_j)\matm{q} = \frac{1}{h^2}\matm{M}\tilde{\vecm{q}} + \sum_j w_j \matm{G}_j^T \matm{p}_j.
\end{equation}

\noindent In the rest of Section \ref{s:pd} we refer to the system matrix of the global system by 
$\matm{S} \coloneqq \frac{1}{h^2}\matm{M} + \sum_j w_j \matm{G}_j^T \matm{G}_j$.

Note that $\matm{S}$ is constant as long as the constraint set remains unchanged. The right side needs to be recomputed in every 
iteration as the projections $\matm{p}_j$ change during the local optimization steps.  An overview over the algorithm is given in 
Algorithm \ref{alg:pd}.

\begin{algorithm}[h]
\caption{Projective Implicit Euler Solver}\label{alg:pd}
\begin{algorithmic}
\Procedure{solvePD}{$\matm{q}_n$, $\matm{v}_n$, $\matm{f}_{\text{ext}}$, $h$}
\State $\tilde{\matm{q}} = \matm{q}_n + h\matm{v}_n + h^2\matm{M}^{-1}\matm{f}_{\text{ext}}$
\State $\matm{q}_k = \tilde{\matm{q}}$
\For{all iterations}
\For{constraints $j$}
\State $\matm{p}_j = \min_{\matm{p}_j} \frac{w_j}{2}\norm{\matm{G}_j\matm{q}_k - \matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j)$
\EndFor
\State $\matm{q}_{k} \gets$ solution of $\matm{S}\matm{q} = \frac{1}{h^2}\matm{M}\tilde{\matm{q}} + \sum_j w_j \matm{G}_j^T \matm{p}_j$.
\EndFor
\State \textbf{return with } $\matm{q}_{n+1} = \matm{q}_k, \matm{v}_{n+1} = (\matm{q}_{n+1} - \matm{q}_n) / h$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Properties of PD}\label{ss:pd-properties}
Projective dynamics is an efficient algorithm for implicit euler integration of the equations of motion with energy potentials of the form 
presented in Equation \ref{eq:pd-potentials}. Its properties are discussed in detail in this section.

\paragraph{Efficient Implementation.}
The structure of the PD energy potentials allows for Algorithm \ref{alg:pd} to be implemented efficiently. Since the constraint projections
in Equation \ref{eq:pd-local-minimization} can be carried out independently, the local optimization step lends itself to massive parallelization. 
Further, because the system matrix $\matm{S}$ is constant, its prefactorization can be computed at initialization, enabling
efficient solves of the linear system in the global optimization step. Note that since $\matm{q} \in \mathbb{R}^{m \times 3}$ in Equation \ref{eq:pd-global-system}, 
the global system can be solved independently and in parallel for each coordinate.

\paragraph{Generality.}
The last property follows from the fact that the squared distance functions $d_j$ in Equation \ref{eq:pd-distance} have no dependencies between 
$x$-, $y$- and $z$-coordinates. This detail demonstrates that restricting to PD energy potentials comes at the cost of generality: 
Many arbitrary nonlinear elastic potentials, particularly those that have dependencies between $x$-, $y$- and $z$-coordinates, cannot be expressed 
in terms of PD elastic potentials. Further, since PD energy potentials $\psi_j$ are defined in terms of a squared distance function $\d_j$, the 
resulting forces are always proportional to the distance from the constraint manifold $\cman{C}_j$ \cite{overby2017}. Many classical energies 
like the Neohookean and St.\ Venant-Kirchhoff energies do not fit into the PD framework \cite{liu2017}.  On the other hand, the authors show that 
a variety of constraints, including strain constraints, bending constraints, collisions and position constraints can be expressed in terms of PD 
potentials and handled by the PD solver in a unified manner \cite{bouaziz2014}. Where applicable, the constraints are derived from continuous 
energies, leaving them reasonably independent to the underlying meshing.

\paragraph{Hard Constraints.}
There is no obvious way to implement hard constraints with the PD solver. However, hard constraints can be emulated by introducing energy potentials 
with arbitrarily large stiffness values, effectively introducing penalty forces to the equations of motion. This approach is discussed in detail in 
Section \ref{ss:penalty-forces}.

\paragraph{Convergence.}
While a simplified minimization problem is constructed by restricting to PD energy potentials, the solver attempts to find the 
true solution of Equation \ref{eq:pd-minimization} instead of computing an approximation. The objective function is quadratic, bounded below 
and both local and global steps are guaranteed to weakly decrease it. As a result, the optimization converges without additional 
safeguards, even if non-convex constraint manifolds are used in the energy potentials. However, this property alone does not ensure that 
PD converges to the positions that minimize the objective function of the variational form of implicit Euler integration. 
According to Bouaziz et al.\ \cite{bouaziz2014}, the true implicit positions preserve linear and angular momentum if the elastic potential 
is rigid motion invariant. It is not clear whether this property is satisfied for the positions that PD converges to as well.

\paragraph{Changing Constraint Sets.}
Lastly, it is important to note that the PD solver is not suited for handling frequently changing constraint sets. For example, every time 
a collision is detected, a new constraint needs to be added to the simulation and the global system matrix $\matm{S}$ needs
to be refactorized. This can slow down the PD solver quite significantly and can lead to unpredictable solver speeds that are infeasible in the
context of real-time simulations.

\subsection{Simulating Deformable Bodies Using PD}\label{ss:pd-deformable-bodies}
As discussed in Section \ref{ss:pd-properties}, classical material models such as the St.\ Venant-Kirchhoff and Neohookean model cannot be expressed in 
terms of PD energy potentials and are incompatible with the PD solver as a result. However, we demonstrate how the strain material model in 
Equation \ref{eq:strain-material-model} can be used with PD. For each tetrahedron in the tetrahedral mesh of the simulated body, we need to 
construct a PD energy potential $\psi_i$ such that 

\[
    \psi_j(\matm{q}) = \min_{\matm{p}_j} \frac{w_j}{2} \norm{\matm{A}_j\matm{q}_j - \matm{B}_j\matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j)
    = V_j\psi_{\text{strain}}(\matm{F}_j),
\]

\noindent where $V_j$ is the volume of the undeformed tetrahedron. Recall that $\matm{F}_j$ is the constant deformation gradient over the volume 
of the tetrahedron and can be computed via Equation \ref{eq:deformation-gradient-tet}. 

Since the energy potential of a single tetrahedron solely depends on its four tetrahedral vertices, it 
is $\matm{q}_j, \matm{p}_j \in \mathbb{R}^{4 \times 3}$. We set the constraint manifold $\cman{C}_j$ to the set of all matrices $\matm{p} 
\in \mathbb{R}^{4 \times 3}$ that correspond to undeformed tetrahedra if the rows of $\matm{p}$ are interpreted as the positions of the 
tetrahedral vertices. Let $\matm{D} \in \mathbb{R}^{3 \times 4}$ be the matrix that maps such matrices $\matm{p}$ to the transpose of their deformation 
gradient $\matm{F}_\matm{p}^T$. Then $\cman{C}_j$ can be defined as

\begin{equation}\label{eq:pd-strain-constraint}
    \cman{C}_j = \{ \matm{p} \mid \matm{D}\matm{p} = \matm{F}_\matm{p}^T \in \text{SO}(3) \},
\end{equation}

\noindent where $\text{SO}(3)$ is the group of three-dimensional rotational matrices. We set $\matm{A}_j = \matm{B}_j = \matm{D}$ and $w_j = k_jV_j$, 
where $V_j$ is the volume of the undeformed tetrahedron and $k_j$ is a user-defined stiffness value. Together, it is

\begin{align}
    \begin{split}\label{eq:pd-strain}    
    \psi_j(\matm{q}) 
    = \min_{\matm{p}_j} W(\matm{S}_j\matm{q}, \matm{p}_j) 
    &= \min_{\matm{p}_j} \frac{k_jV_j}{2} \norm{\matm{D}\matm{q}_j - \matm{D}\matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j) \\
    &= \min_{\matm{F}_{\matm{p}_j}} \frac{k_jV_j}{2} \norm{\matm{F}_j^T - \matm{F}_{\matm{p}_j}^T}^2_F + \delta_{\text{SO}(3)}(\matm{F}_{\matm{p}_j}).
    \end{split}
\end{align}

\noindent It is easy to show that $\matm{F}_{\matm{p}_j}$ can be computed as $\matm{F}_{\matm{p}_j} = \matm{UIV}^T$ where $\matm{F}_j = 
\matm{U \Sigma V}^T$ is the singular value decomposition of the deformation gradient $\matm{F}_j$. Plugging into Equation \ref{eq:pd-strain}
yields

\[
    \psi_j(\matm{q}) = \frac{k_jV_j}{2} \norm{\matm{U}^T\matm{\Sigma}\matm{V} - \matm{U}^T\matm{I}\matm{V}}^2_F 
    = \frac{k_jV_j}{2} \norm{\matm{\Sigma} - \matm{I}}^2_F = V_j \psi_{\text{strain}}(\matm{F}_i),
\]

\noindent showing that the strain material model can be expressed in terms of PD energy potentials.

\subsection{PD as a Special Case of Quasi-Newton Methods}\label{ss:pd-quasi-newton}
In PD, the variational form of implicit Euler integration that results from restricting to PD energy potentials (see Eq.\ \ref{eq:pd-minimization}) 
is solved by using a specialized local/global alternating minimization technique (see Sec.\ \ref{ss:pd-solver}). If the projection points 
$\matm{p}^j_{\matm{q}}$ with 

\[
    \matm{p}_{\matm{q}_j} 
    = \argmin{\matm{p}_j} \psi_j(\matm{q}) 
    = \argmin{\matm{p}_j} \frac{w_j}{2} \norm{\matm{G}_j\matm{q} - \matm{p}_j}^2_F + \delta_{\cman{C}_j}(\matm{p}_j)
\]

\noindent are considered functions of $\matm{q}$ with $\matm{p}_j(\matm{q}) = \matm{p}^j_{\matm{q}}$, then the equivalent optimization problem

\begin{equation}\label{eq:pd-minimization-q}
    \min_{\matm{q}} g(\matm{q}) = 
    \min_{\matm{q}} \frac{1}{2h^2} \norm{\matm{M}^{1/2}(\matm{q} - \tilde{\matm{q}})}^2_F + \sum_j \frac{w_j}{2} \norm{\matm{G}_j\matm{q}
    - \matm{p_j}(\matm{q})}^2_F
\end{equation}

\noindent can be solved using general purpose algorithms for unconstrained optimization. We show that the PD solver can be interpreted as a Quasi-Newton 
method applied to the objective function $g$ in Equation \ref{eq:pd-minimization-q} \cite{liu2017}.

We start by briefly introducing Newton's method, which is one of the most popular algorithms for unconstrained optimization. For a more detailed introduction, 
we refer the interested reader to the relevant chapters in the textbook on numerical optimization by Nocedal and Wright \cite{nocedal2006}. Newton's method 
is an iterative procedure that starts with some initial guess $\matm{q}_0$ and then iteratively improves it by applying updates in a direction that is 
guaranteed to reduce the objective function until a local minimum is achieved. Applying Newton's method to the objective function $g$ in Equation 
\ref{eq:pd-minimization-q} yields the following update formula 

\begin{equation}\label{eq:pd-newton-update}
    \matm{q}_{k+1} = \matm{q}_k - \alpha_k \nabla^2 g(\matm{q}_k)^{-1} \nabla g(\matm{q}_k) = \matm{q}_k - \alpha_k \matm{r}^N_k.
\end{equation}

\noindent Here, $\nabla^2 g(\matm{q}_k)$ is the Hessian matrix of $g$ at $\matm{q}_k$, $\matm{r}^N_k = \nabla^2 g(\matm{q}_k)^{-1} \nabla g(\matm{q}_k)$ 
is called the Newton direction and $\alpha_k \in \mathbb{R}^+$ is called the step size. If $\nabla^2 g(\matm{q}_k)$ is invertible and $\alpha_k$ is 
sufficiently small, the Newton update is guaranteed to decrease the objective function. However, to ensure that each Newton update makes sufficient progress 
towards a local minimum, it is desirable to keep $\alpha_k$ as large as possible. Newton's method exhibits favorable convergence properties, but the computation 
and storage of the inverse Hessian matrix $\nabla^2 g(\matm{q}_k)^{-1}$ is often prohibitively expensive. This gives rise to the so-called Quasi-Newton methods. 
There, the Hessian matrix $\nabla^2 g(\matm{q}_k)$ is approximated by some matrix $\matm{B}_k$ that is faster to compute, but still yields suitable search directions 
$\matm{r}_k$. The update formula for Quasi-Newton methods is given by

\begin{equation}\label{eq:pd-qn-update}
    \matm{q}_{k+1} = \matm{q}_k - \alpha_k \matm{B}^{-1}_k \nabla g(\matm{q}_k).
\end{equation}

\begin{algorithm}[b]
\caption{Projective Dynamics as a Quasi-Newton Method}\label{alg:pd-qn}
\begin{algorithmic}
\Procedure{solvePDViaQN}{$\matm{q}_n$, $\matm{v}_n$, $\matm{f}_{\text{ext}}$, $h$}
\State $\tilde{\matm{q}} = \matm{q}_n + h\matm{v}_n + h^2\matm{M}^{-1}\matm{f}_{\text{ext}}$
\State $\matm{q}_k = \tilde{\matm{q}}$
\For{all iterations}
\State $\matm{p}_k \gets$ solution of $\matm{S}\matm{p} = -\nabla g(\matm{q}_k)$
\State $\matm{q}_k = \matm{q}_k + \matm{p}_k$
\EndFor
\State \textbf{return with result } $\matm{q}_{n+1} = \matm{q}_k, \matm{v}_{n+1} = (\matm{q}_{n+1} - \matm{q}_n)/h$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now, consider the Quasi-Newton update formula with constant step size $\alpha_k = 1$ and Hessian approximation $\matm{B}_k = \matm{S}$, where $\matm{S}$
is the system matrix from the global optimization in PD (see Eq.\ \ref{eq:pd-global-system}), given by

\begin{equation}\label{eq:pd-qn-update-S}
    \matm{q}_{k+1} = \matm{q}_k - \matm{S}^{-1}_k \nabla g(\matm{q}_k).
\end{equation}

\noindent Liu et al. \cite{liu2017} show that $\nabla g$ is equal to

\begin{equation}\label{eq:pd-gradient-qn}
    \nabla g(\matm{q}) = \frac{1}{h^2}\matm{M}(\matm{q} - \tilde{\matm{q}}) + \sum_j w_j \matm{G}^T_j \matm{G}_j \matm{q}
    - \sum_j w_j \matm{G}^T_j \matm{p}_j(\matm{q}).
\end{equation}

\noindent Surprisingly, this is the same as the gradient $\nabla_{\matm{q}} \tilde{g}(\matm{q}, \matm{p}_j)$ of the global optimization problem of the original
PD algorithm (see Eq.\ \ref{eq:pd-gradient-q}). Thus, the gradient is unaffected by whether $\matm{p}_j$ is considered constant or a function
$\matm{p}_j(\matm{q})$ of the positions $\matm{q}$. Plugging $\nabla g(\vecm{q}_k)$ into Equation \ref{eq:pd-qn-update-S} results in 

\begin{align*}
    \matm{q}_{k+1} &= \matm{q}_k - \matm{S}^{-1}\left(\frac{1}{h^2}\matm{M}(\matm{q}_k - \tilde{\matm{q}}) + \sum_j w_j \matm{G}^T_j \matm{G}_j \matm{q}_k
    - \sum_j w_j \matm{G}^T_j \matm{p}_j(\matm{q}_k)\right)\\
                   &= \matm{q}_k - \matm{S}^{-1}\left(\matm{S}\matm{q}_k - \frac{1}{h^2}\tilde{\matm{q}} - \sum_j w_j \matm{G}^T_j \matm{p}_j(\matm{q}_k)\right)\\
                   &= \matm{S}^{-1}\left(\frac{1}{h^2}\tilde{\matm{q}} + \sum_j w_j \matm{G}^T_j \matm{p}_j(\matm{q}_k)\right).
\end{align*}

\noindent Note that this is exactly the solution of the global linear system from PD in Equation \ref{eq:pd-global-system}. Together, this shows 
that performing a Quasi-Newton step with Hessian approximation $\matm{B}_k = \matm{S}$ and step size $\alpha_k = 1$ on the 
minimization problem in Equation \ref{eq:pd-minimization-q} is equivalent to performing a local/global iteration of the PD solver 
(\cref{alg:pd}). The Quasi-Newton version of PD is summarized in Algorithm \ref{alg:pd-qn}.

The insight that PD is a special case of Quasi-Newton methods applied to Equation \ref{eq:pd-minimization-q} can be leveraged to
bring performance improvements to the original PD solver and to design a natural extension of PD to energies that do not fit 
into the original PD framework. Both are discussed in Section \ref{s:qn-rts}.

\section{Quasi-Newton Methods for Physical Simulations}\label{s:qn-rts}
In Section \ref{ss:pd-quasi-newton}, we discussed that the PD solver for the minimization problem in Equation \ref{eq:pd-minimization}
can be interpreted as a special case of a Quasi-Newton method with constant Hessian approximation and step size $\alpha = 1$
applied to the corresponding minimization problem Equation \ref{eq:pd-minimization-q}. The formulation that is suitable for Quasi-Newton
methods

\begin{equation}\label{eq:pd-minimization-q2}
    \min_{\matm{q}} g(\matm{q}) = 
    \min_{\matm{q}} \frac{1}{2h^2} \norm{\matm{M}^{1/2}(\matm{q} - \tilde{\matm{q}})}^2_F + \sum_j \frac{w_j}{2} \norm{\matm{G}_j\matm{q}
    - \matm{p_j}(\matm{q})}^2_F
\end{equation}

\noindent and the global system matrix $\matm{S}$ serving as the constant Hessian approximation

\begin{equation}\label{eq:global-matrix}
    \matm{S} \coloneqq \frac{1}{h^2}\matm{M} + \sum_j w_j \matm{G}_j^T \matm{G}_j  
\end{equation}

\noindent are restated for convenience of the reader. This highlights one of the weaknesses of the PD solver. Since the Hessian approximation 
is constant, the PD solver does not take advantage of local curvature information around the current positions $\vecm{q}_k$ during its position 
update. This is in contrast to Newton's method, where the curvature of the objective function around $\vecm{q}_k$ is captured by the Hessian 
matrix $\nabla^2 g(\matm{q}_k)$ and is used to find an effective search direction. Most Quasi-Newton methods aim at computing Hessian 
approximations $\matm{B}_k$ that are as close to $\nabla^2 g(\matm{q}_k)$ as possible and capture the local curvature of the objective function 
reasonably well as a result. The L-BFGS method is a Quasi-Newton method that achieves this by computing a Hessian approximation $\matm{B}_k$ 
from some initial Hessian approximation $\matm{B}_0$, the previous $l \in \mathbb{N}^+$ iterates $\matm{q}_{k-1}, \ldots, \matm{q}_{k-l}$ 
and their gradients $\nabla g(\matm{q}_{k-1}), \ldots, \nabla g(\matm{q}_{k-l})$ \cite{nocedal2006}. The choice of the initial Hessian 
approximation $\matm{B}_0$ has a strong impact on the performance of the L-BFGS method. Usually, a scaled version of the identity matrix is 
used. Liu et al.\ \cite{liu2017} suggest that setting $\matm{B}_0 = \matm{S}$ combines the strengths of the PD solver and L-BFGS method 
and gives rise to a powerful approach to implicit Euler integration when applied to the optimization problem in Equation \ref{eq:pd-minimization-q2}.
In the rest of this thesis, we refer to the resulting solver as the QN solver. It is discussed in detail in Section \ref{ss:qn-pd}.

Since Quasi-Newton methods can be applied to the variational form of implicit Euler integration with general conservative energy potentials 
(see Eq.\ \ref{eq:variational-implicit}), this suggests a natural extension of the approach mentioned above to energies that do not fit into 
the original PD framework. However, if general conservative energy potentials are used, it is not obvious how to construct the initial Hessian 
approximation $\matm{S}$ anymore. Liu et al.\ suggest a way to emulate the global system matrix $\matm{S}$ for energies that can be written in
the Valanis-Landel form, which includes Neohookean and St.\ Venant-Kirchhoff energies. This extension is covered in Section 
\ref{ss:qn-valanis-landel}. Finally, the properties of the QN solver are discussed in Section \ref{ss:properties-qn}.


\subsection{Quasi-Newton Methods for PD Energy Potentials}\label{ss:qn-pd}
An overview over the L-BFGS method with initial Hessian approximation $\matm{B}_0 = \matm{S}$ applied to the optimization problem in Equation 
\ref{eq:pd-minimization-q2} is given in Algorithm \ref{alg:pd-lbfgs}. For more details on the L-BFGS method, we again refer to the book 
on numerical optimization by Nocedal and Wright \cite{nocedal2006}. To start off the L-BFGS solver, an initial guess $\matm{q}_0$ for a local 
minimizer of the objective function in Equation \ref{eq:pd-minimization-q2} is required. In the QN solver, the initial guess is set to the 
inertial positions, i.e.\ $\matm{q}_0 = \tilde{\matm{q}}$ (lines 3-4). For each iteration $k$, the search direction $\matm{p}_k = 
-\matm{B}_k^{-1}\nabla g(\matm{q}_k)$ is computed in lines 6-7. In lines 8-12, an appropriate step size $\alpha_k$ for the current search 
direction $\matm{p}_k$ is determined. Given the step size $\alpha_k$ and the search direction $\matm{p}_k$, the next iterate 
is computed in line 12. Finally, the positions and the velocities for the next time step are updated after the last iteration in line 15.

\begin{algorithm}[t]
\caption{L-BFGS Method For PD Energies}\label{alg:pd-lbfgs}
\begin{algorithmic}[1]
\State \textbf{require } $\beta \in (0, 1)$, $t \in (0, 1)$
\Procedure{solvePDViaLBFGS}{$\matm{q}_n$, $\matm{v}_n$, $\matm{f}_{\text{ext}}$, $m$, $h$}
\State $\tilde{\matm{q}} = \matm{q}_n + h\matm{v}_n + h^2\matm{M}^{-1}\matm{f}_{\text{ext}}$
\State $\matm{q}_k = \tilde{\matm{q}}$
\For{all iterations}
\State $\matm{s_k} = \matm{q}_k - \matm{q}_{k-1}, \matm{y}_k = \nabla g(\matm{q}_k) - \nabla g(\matm{q}_{k-1}), \rho_k = 1 / \text{tr}(\matm{s}^T_k \matm{y}_k)$
\State $\matm{p}_k = \text{\textsc{twoLoopRecursion}}(\matm{S}, \matm{q}_k, \matm{s}, \matm{y}, \rho, m, k)$   (\cref{alg:lbfgs-recursion})
\State $\alpha_k = 1$
\If{$\alpha_k$ does not satisfy the strong Wolfe conditions}
\State compute $\alpha_k$ that satisfies the strong Wolfe conditions 
\State \textbf{or} $\alpha_k = \text{\textsc{backtrack}}(\matm{q}_k, \matm{p}_k, 1, \beta, t)$ (\cref{alg:backtracking})
\EndIf
\State $\matm{q}_k = \matm{q}_k + \alpha_k \matm{p}_k$
\EndFor
\State \textbf{return with result } $\matm{q}_{n+1} = \matm{q}_k, \matm{v}_{n+1} = (\matm{q}_{n+1} - \matm{q}_n)/h$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The search direction $\matm{p}_k$ is computed using the two-loop recursion shown in Algorithm \ref{alg:lbfgs-recursion} \cite{nocedal2006, liu2017}.
Here, the Hessian approximation used in the L-BFGS method $\matm{B}_k$ does not need to be stored or computed explicitly. Instead, the 
search direction $\matm{p}_k$ can be calculated directly at the cost of $\mathcal{O}(l)$ inner products and vector additions (lines 3-5 and 8-10) 
and solving an LSE with constant system matrix $\matm{S}$ (line 7). The quantities $\matm{s}_k, \matm{y}_k$ and $\rho_k$ are computed from the 
positions and gradients from previous iterations and are stored over a window of $l$ iterations (see Alg.\ \ref{alg:pd-lbfgs}, line 6). This 
enables capturing curvature information during the computation of the search direction $\matm{p}_k$ in the two-loop recursion. 

\begin{algorithm}[tb]
\caption{L-BFGS Two-Loop Recursion}\label{alg:lbfgs-recursion}
\begin{algorithmic}[1]
\Procedure{twoLoopRecursion}{$\matm{B}_0$, $\matm{q}_k$, $\matm{s}$, $\matm{y}$, $\rho$, $l$, $k$}
\State $l^* = \min(l, k), \matm{t} = -\nabla g(\matm{q}_k)$
\For{$i = k-1, k-2, \ldots, k-l^*$}
\State $\alpha_i = \rho_i \text{tr}(\matm{s}^T_i \matm{t})$
\State $\matm{t} = \matm{t} - \alpha_i \matm{y}_i$
\EndFor
\State solve $\matm{B}_0 \matm{r} = \matm{t}$
\For{$i = k-l^*, k-l^*+1, \ldots, k-1$}
\State $\beta = \rho_i \text{tr}(\matm{y}^T_i \matm{r})$
\State $\matm{r} = \matm{r} + \matm{s}_i(\alpha_i - \beta)$
\EndFor
\State \textbf{return with result } -$\matm{B}^{-1}_k \nabla g(\matm{q}_k) = \matm{r}$.
\EndProcedure
\end{algorithmic}
\end{algorithm}

Step sizes $\alpha_k$ for the L-BFGS method are usually chosen such that they satisfy the Wolfe conditions, which ensures that sufficient progress 
towards a local minimizer is made during each L-BFGS iteration (see \cite{nocedal2006}). Finding such step sizes typically requires the execution of 
so-called step size selection algorithms. Instead, Liu et al.\ \cite{liu2017} opt for a simpler strategy called backtracking (see Alg.\ 
\ref{alg:backtracking}). There, the natural step size associated with Newton's method $\alpha_k = 1$ is iteratively reduced by a factor of $\beta$ 
until the first Wolfe condition is satisfied (lines 4-5). This is guaranteed to happen if $\matm{p}_k$ is a descent 
direction and if $\alpha_k$ is sufficiently small. In practice, backtracking is stopped after $\alpha_k < t$ for some user defined threshold $t$ to 
prevent pathologically small step sizes (line 4).


\subsection{Quasi-Newton Methods for Valanis-Landel Energies}\label{ss:qn-valanis-landel}
To achieve satisfactory performance when applying the L-BFGS method to the variational form of implicit Euler integration 
(see Eq.\ \ref{eq:variational-implicit}) without the need to restrict energy potentials to the PD framework (see Eq.\ \ref{eq:pd-potentials}), a suitable
candidate for the initial Hessian approximation $\matm{B}_0$ or its inverse $\matm{H}_0$ needs to be found. The choice $\matm{B}_0 = 
\matm{S} = \frac{1}{h^2}\matm{M} + \sum_j w_j \matm{G}_j^T \matm{G}_j$ from Algorithm \ref{alg:pd-lbfgs} cannot be applied to the general setting 
without modification as the matrices $\matm{G}_j$ are taken directly from the definitions of the individual PD energy potentials.


We focus on the setting of tetrahedral meshes with energy potentials that are derived from one of the material models in Section \ref{ss:material-models} 
for each tetrahedron. Here, the energy of the entire body is the sum of the individual tetrahedral energies (see Sec.\ \ref{ss:deformable-bodies}).
As shown in Section \ref{ss:pd-deformable-bodies}, the energy of a single tetrahedron can be emulated using PD potentials when the strain material model is 
used (see Eq.\ \ref{eq:pd-strain}). Here, $\matm{G}_j = \matm{A}_j$, where $\matm{A}_j$ is the linear operator that maps $\matm{q}$ 
to the transpose of the deformation gradient $\matm{F}^T_j$, $\cman{C}_j = \text{SO}(3)$ and $w_j = k_jV_j$, where $V_j$ is the
volume of the undeformed tetrahedron and $k_j$ is a user-defined stiffness value. 
Liu et al.\ suggest approximating more general tetrahedral energies with PD strain potentials and using the global system
matrix $\matm{S}$ that arises as the initial Hessian approximation $\matm{B}_0$ given by

\begin{equation}\label{eq:global-matrix-strain}
    \matm{B}_0 = \matm{S} = \frac{1}{h^2}\matm{M} + \sum_j w_j \matm{A}_j^T \matm{A}_j.
\end{equation}

Instead of using user-defined stiffness values $k_j$, a procedure for setting $k_j$ from the original, more general energy
potential is required. The discussion is restricted to isotropic and world-space rotation invariant material models that 
can be defined in terms of the singular 
values $\sigma_1, \sigma_2, \sigma_3$ of $\matm{F_j}$ called the principal stretches \cite{sifakis2012}. Liu et al.\ 
\cite{liu2017} present a strategy for a subclass of these material models that can be written in Valanis-Landel form

\begin{equation}\label{eq:valanis-landel}
    \Psi(\sigma_1, \sigma_2, \sigma_3) = a(\sigma_1) + a(\sigma_2) + a(\sigma_3) + b(\sigma_1 \sigma_2) 
    + b(\sigma_2 \sigma_3) + b(\sigma_1 \sigma_3)
\end{equation}

\noindent with $a, b, c : \mathbb{R} \to \mathbb{R}$. Many popular material models including the St.\ Venant-Kirchhoff model and 
the Neohookean model can be expressed in this form.

\begin{algorithm}[t]
\caption{Backtracking Line Search}\label{alg:backtracking}
\begin{algorithmic}[1]
\State \textbf{require } $\tilde{\alpha} > 0, c_1 \in (0, 1), \beta \in (0, 1), t \in (0, \tilde{\alpha})$
\Procedure{backtrack}{$\matm{q}_k$, $\matm{p}_k$, $\tilde{\alpha}$, $\beta$, $t$}
\State $\alpha = \tilde{\alpha}$
\While{$g(\matm{q}_k + \alpha \matm{p}_k) > g(\matm{q}_k) + c_1 \alpha \nabla g(\matm{q}_k)^T \matm{p}_k$ and $\alpha > t$}
\State $\alpha = \beta \alpha$
\EndWhile
\State \textbf{return with } $\alpha_k = \alpha$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The approach uses the insight that for linear materials that follow Hooke's law, the stiffness $k_j$ is given as the second 
derivative of the energy potential. Computing first and second partial derivatives in the rest configuration $\sigma_1, 
\sigma_2, \sigma_3 = 1$ yields

\begin{align}
    \evalat[\bigg]{\frac{d \psi}{d \sigma_i}}{\sigma_j, \sigma_k = 1} 
    &= a^{\prime}(\sigma_i) + 2b^{\prime}(\sigma_i) + c^{\prime}(\sigma_i) \label{eq:valandis-landel-first} \\
    \evalat[\bigg]{\frac{d^2 \psi}{d \sigma_i}}{\sigma_j, \sigma_k = 1} 
    &= a^{\prime\prime}(\sigma_i) + 2b^{\prime\prime}(\sigma_i) + c^{\prime\prime}(\sigma_i) \label{eq:valandis-landel-second},
\end{align}

\noindent where $i, j, k \in [1, 3], i \neq j \neq k$. Thus, the first and second partial derivatives at the rest configuration are the same
for each of the principal stretches, allowing for convenient representation of the material stiffness in a single scalar value. However,
simply picking

\[
    k_j = \evalat[\bigg]{\frac{d^2 \psi}{d \sigma_i}}{\sigma_i, \sigma_j, \sigma_k = 1} 
    &= a^{\prime\prime}(1) + 2b^{\prime\prime}(1) + c^{\prime\prime}(1)
\]

\noindent runs into issues as the expression often evaluates to zero, even in common non-pathological cases. This issue arises because
the second derivative in Equation \ref{eq:valandis-landel-second} is not representative of the stiffness behavior of the material in the 
entire range of principal stretches $[\sigma_{\text{min}}, \sigma_{\text{max}}]$ that is expected to be encountered during the simulation.

To alleviate this, Liu et al.\ \cite{liu2017} propose approximating the rate of change of the first derivative Equation \ref{eq:valandis-landel-first}
by computing the slope of its best linear approximation over the interval $[\sigma_{\text{min}}, \sigma_{\text{max}}]$. Formally, $k$ is 
defined by

\begin{equation}\label{eq:valanis-landel-k}
    k \coloneqq \argmin{k} \int^{\sigma_{\text{max}}}_{\sigma_{\text{min}}} (k(\sigma - 1) - (a^{\prime}(\sigma) + 2b^{\prime}(\sigma) + 
    c^{\prime}(\sigma))^2 d\sigma
\end{equation}

\noindent Using these stiffness values in Equation \ref{eq:global-matrix-strain} yields a suitable initial Hessian approximation $\matm{B}_0 = S$ for
the minimization of the variational form of the implicit Euler integration with Valanis-Landel materials via Algorithm \ref{alg:pd-lbfgs}. According
to Liu et al.\ Algorithm \ref{alg:pd-lbfgs} is insensitive to the size of the interval $[\sigma_{\text{min}}, \sigma_{\text{max}}]$. It is important
to note that while PD strain energies are used to approximate the original energy potentials when constructing $\matm{B}_0$, the function
evaluations $g(\matm{q}_k)$ and gradients $\nabla g(\matm{q}_k)$ are computed from the original potential energies in Algorithm \ref{alg:pd-lbfgs}.

\subsection{Properties of the QN Solver}\label{ss:properties-qn}
The QN solver described in Sections \ref{ss:qn-pd} and \ref{ss:qn-valanis-landel} emerges from combining ideas from PD and the L-BFGS method for 
unconstrained optimization. As a result, the QN solver has superior convergence properties and is more general than the PD solver. We discuss the 
properties of the QN solver below.

\paragraph{Relation to the PD Solver.}
As described in Section \ref{ss:pd-quasi-newton}, the PD solver can be interpreted as a Quasi-Newton method with constant step size and Hessian approximation 
applied to the optimization problem in Equation \ref{eq:pd-minimization-q2}. The QN solver enhances the PD solver by applying L-BFGS updates that capture local
curvature information to the constant Hessian approximation $\matm{S}$ and performing backtracking to find a step size that is guaranteed to at least satisfy 
the first Wolfe condition. Experimental data provided by Liu et al.\ \cite{liu2017} suggests that this leads to 
significantly improved convergence properties. This is in line with the fact that the convergence properties of Quasi-Newton methods benefit from Hessian 
approximations $\matm{B}_k$ that are as close as possible to the true Hessian matrix $\nabla^2 g(\matm{q}_k)$ \cite{nocedal2006}. As described in Section 
\ref{ss:qn-valanis-landel}, the QN solver can be applied to material models that can be written in the Valanis-Landel form. Thus, the QN solver is 
more general than the PD solver.

\paragraph{Step Size.}
In our discussion of PD (see Sec.\ \ref{ss:pd-properties}), we pointed out that PD iterations are guaranteed to weakly decrease the objective function of 
the variational form of implicit Euler integration. As such, using a constant step size of $\alpha_k = 1$ is a viable choice when the constant Hessian 
approximation $\matm{B}_k = \matm{S}$ is chosen. In contrast, it is well known that search directions $\matm{p}_k$ produced by the L-BFGS method may not 
be directions of descent if the step sizes $\alpha_k$ are not guaranteed to satisfy the Wolfe conditions \cite{nocedal2006}. In such settings, convergence 
of the L-BFGS method 
is not guaranteed. Since the step size $\alpha_k$ determined by backtracking (see Alg.\ \ref{alg:pd-lbfgs}) may not satisfy the second Wolfe 
condition, this applies for the QN solver as well. However, Liu et al.\ \cite{liu2017} report that skipping more complicated step size selection algorithms 
in favor of simple backtracking works well in practice. In contrast, leaving out backtracking all together and simply using $\alpha_k=1$ produces poor 
results according to the authors. Thus, it is essential that step sizes at the very least satisfy the first Wolfe condition.

\paragraph{Efficient Implementation.}
The QN solver lends itself to efficient implementation. Firstly, the contributions of each tetrahedral element to the evaluations of the objective function $g$ 
and its gradient $\nabla g$ can be computed independently and are easily parallelizable as a result. Secondly, the two-loop recursion (see Alg.\ 
\ref{alg:lbfgs-recursion}) can be performed at the cost of $\mathcal{O}(l)$ inner products and vector additions (lines 3-5 and 8-10) and solving an LSE with 
constant system matrix $\matm{S}$ (line 7). The total computational cost incurred by the inner products 
and vector additions is only $\mathcal{O}(lm)$, where $m$ is the number of particles. Solving the LSE can be sped up by precomputing and storing a matrix 
factorization of $\matm{S}$. Lastly, the LSE can be solved independently for $x$-, $y$- and $z$- coordinates since $\matm{t} \in \mathbb{R}^{m \times 3}$.

\paragraph{Choice of L-BFGS History Window Size.}
The user defined L-BFGS window size $l$ needs to be large enough to sufficiently capture local curvature information. On the other hand, L-BFGS performance can 
be negatively affected by taking into account distant iterates if $l$ is too large. Liu et al.\ \cite{liu2017} recommend a window size of $l = 5$. Additionally, the 
authors report that it may be useful to reduce the window size for simulations with frequent collisions.

\paragraph{Changing Constraint Sets.}
In PD, the system matrix $\matm{S}$ and its factorization need to be recomputed each time a constraint is added to or removed from the simulation. This is not the case 
for the initial Hessian approximation $\matm{B}_0$ used in the QN solver \cite{liu2017}. When a new constraint is added, it is accounted for by the contribution of its 
energy or energy gradient to the evaluation of $g$ and $\nabla g$. Ignoring its contribution to the initial matrix approximation $\matm{B}_0$ can be interpreted as a more 
aggressive Hessian approximation that might negatively affect the convergence rate of the solver. However, with an an appropriate step size selection algorithm in place, 
the solver will still successfully decrease the objective function $g$. This suggests that it might be helpful to replace backtracking with a step size selection 
algorithm that ensures that the step size $\alpha_k$ satisfies both Wolfe conditions during simulations with rapidly changing constraint sets. This includes collision 
heavy simulations.

\paragraph{Hard Constraints.}
The challenges of implementing hard constraints with the QN solver are identical to those discussed for the PD solver in Section \ref{ss:pd-properties}.

\section{Alternating Direction Method of Multipliers}\label{s:admm}
Both the PD solver covered in Section \ref{s:pd} and the QN solver introduced in Section \ref{s:qn-rts} perform implicit Euler integration by minimizing the objective 
function of the variational form of implicit Euler integration

\begin{equation}\label{eq:variational-implicit-copy-admm}
    \min_{\vecm{q}_{n+1}} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\vecm{q}_{n+1} - \tilde{\vecm{q}})}^2_F + \sum_j \psi_j(\vecm{q}_{n+1}).
\end{equation}

\noindent Since PD can be interpreted as a Quasi-Newton method with constant step size and Hessian approximation (see Sec.\ \ref{ss:pd-quasi-newton}), both of these 
solvers approach the minimization problem in Equation \ref{eq:variational-implicit-copy-admm} through the lense of unconstrained optimization. It turns out that 
Equation \ref{eq:variational-implicit-copy-admm} can be expressed as an equivalent constrained optimization problem of the form 

\begin{align}\label{eq:admm-constrained-problem}
    \min_{\vecm{q}, \vecm{z}}\, &f(\vecm{q}) + g(\vecm{z}) \\ 
    \text{subject to } &\matm{A}\vecm{q} + \matm{B}\vecm{z} = \vecm{c}
\end{align}

\noindent for appropriate functions $f, g$, matrices $\matm{A}, \matm{B}$ and vectors $\vecm{q}, \vecm{z}, \vecm{c}$. This allows approaching implicit 
Euler integration using tools from the constrained optimization literature. Overby et al.\ \cite{overby2017} propose performing implicit Euler integration by solving the 
corresponding constrained optimization problem by a popular algorithm for constrained optimization called the Alternating Direction Method of Multipliers (ADMM). We refer 
to the resulting solver as the ADMM solver. The ADMM solver is highly parallelizable, can be applied to arbitrary conservative forces and supports hard constraints. 
Overby et al.\ \cite{overby2017} show that the ADMM solver is equivalent to PD in the special case of linear forces. Thus, similar to the QN solver, ADMM can be considered 
an extension of PD. We describe how to apply ADMM to the variational form of implicit Euler integration in Section \ref{ss:admm-solver}. In Section \ref{ss:admm-hard-constraints}, 
we outline how hard constraints can be implemented with the ADMM solver. In Section \ref{ss:admm-properties}, the properties of the ADMM solver are discussed. 

\subsection{ADMM Solver}\label{ss:admm-solver}
ADMM is an algorithm for solving constrained optimization problems of the form given in Equation \ref{eq:admm-constrained-problem}. The algorithm takes initial values 
$\vecm{q}_0$, $\vecm{z}_0$ and $\vecm{u}_0$ and iteratively updates them according to the following formulas \cite{gallier2020}

\begin{align}\label{eq:admm-general}
    \begin{split}
        \vecm{q}_{k+1} &= \argmin{\vecm{q}} \left( f(\vecm{q}) + \frac{\rho}{2}\norm{\matm{A}\vecm{q} + \matm{B}{z}_k - \vecm{c} + \vecm{u}_k}^2 \right)\\ 
        \vecm{z}_{k+1} &= \argmin{\vecm{z}} \left( g(\vecm{z}) + \frac{\rho}{2}\norm{\matm{A}\vecm{q}_{k+1} + \matm{B}\vecm{z} - \vecm{c} + \vecm{u}_k}^2 \right)\\
        \vecm{u}_{k+1} &= \vecm{u}_{k} + (\matm{A}\vecm{q}_{k+1} + \matm{B}\vecm{z}_{k+1} - \vecm{c}).
    \end{split}
\end{align}

\noindent Here, $\vecm{u}$ is an auxiliary scaled dual variable, $\rho \in \mathbb{R}^+$ is a step length parameter and the subscripts denote the current iteration.

To perform implicit Euler integration using ADMM, we express the variational form of implicit Euler integration (see Eq. \ref{eq:variational-implicit-copy-admm}) in 
terms of a constrained optimization problem of the form in Equation \ref{eq:admm-constrained-problem} \cite{overby2017}. To achieve this, we rewrite the individual energy terms 
$\psi_j$ in terms of local variables $\vecm{z}_j = \matm{D}_j\vecm{q}$, where $\matm{D}_j$ is a so-called reduction matrix. Then, the energy potential 
$\Psi$ can be rewritten as follows

\[
    \Psi({\vecm{q}}) = \sum_j \psi_j(\matm{D}_j \vecm{q}) = \sum_j \psi_j(\vecm{z}_j).
\]

\noindent As an example, if $\psi_j(\matm{D}_j\vecm{q})$ measures the elastic energy of tetrahedron $j$ at configuration $\vecm{q}$ according to one of the material 
models introduced in Section \ref{ss:material-models}, then $\matm{D}_j$ can be chosen such that $\matm{D}_j\vecm{q} = \text{flatten}(\matm{F}_j)$, where $\matm{F}_j$
is the deformation gradient of tetrahedron $j$. If the individual reduction matrices $\matm{D}_j$ are stacked into a single matrix 
$\matm{D} = [\matm{D}^T_1, \ldots, \matm{D}^T_r]^T$ the concatenation of the local coordinates $\vecm{z} = [\vecm{z}_1, \ldots, \vecm{z}_r]$ is given by 
$\vecm{z} = \matm{D}\vecm{q}$. This allows introducing the function $\Psi_*$ with 

\[
    \Psi_*(\matm{D}\vecm{q}) = \sum_j \psi_j (\matm{D}_j\vecm{q}) = \Psi(\vecm{q}).
\]

\noindent Then, the variational form of implicit Euler integration can be rewritten as follows

\begin{equation}\label{eq:variational-admm}
    \min_{\vecm{q}_{n+1}} \frac{1}{2h^2} \norm{\matm{M}^{\frac{1}{2}}(\vecm{q}_{n+1} - \tilde{\vecm{q}})}^2_F + \Psi_*(\matm{D}\vecm{q}_{n+1}).
\end{equation}

\noindent In the context of the ADMM solver, $\vecm{q} \in \mathbb{R}^{3m}$ and $\matm{M} \in \mathbb{R}^{3m \times 3m}$, where $m \in \mathbb{N}^+$ is the 
number of particles. By setting 

\begin{align}\label{eq:admm-translation}
    \begin{split}
        f(\vecm{q}) &= \frac{1}{2h^2}\norm{\matm{M}^{\frac{1}{2}}(\vecm{q} - \tilde{\vecm{q}})}^2, \\
        g(\vecm{z}) &= \Psi_*(\vecm{z}), \\ 
        \matm{A} &= \matm{W}\matm{D}, \\ 
        \matm{B} &= -\matm{W}, \\
        \vecm{c} &= \vecm{0},
    \end{split}
\end{align}

\noindent for any invertible weight matrix $\matm{W}$, the variational form of implicit Euler integration can be expressed in terms of the constrained optimization problem 
in Equation \ref{eq:admm-constrained-problem}. Typically, $\matm{W}$ is chosen to be block diagonal with blocks $\matm{W}_j$ of the same size as the corresponding local 
vectors $\vecm{z}_j$. In this thesis, we use $\matm{W}_j = w_j \matm{I}$ for user defined weights $w_j \in \mathbb{R}^+$.

Plugging the identities from Equation \ref{eq:admm-translation} into the ADMM update formulas in Equation \ref{eq:admm-general} yields 

\begin{align}\label{eq:admm-solver}
    \vecm{q}_{k+1} &= (\matm{M} + \rho h^2 \matm{D}^T\matm{W}^T\matm{W}\matm{D})^{-1} 
    \left( \matm{M}\tilde{\vecm{q}} + \rho h^2 \matm{D}^T\matm{W}^T\matm{W}(\vecm{z}_k - \bar{\vecm{u}}_k) \right), \label{eq:admm-q-update} \\
    \vecm{z}_{k+1} &= \argmin{\vecm{z}} \left( \Psi_*(\vecm{z}) + \frac{\rho}{2}\norm{\matm{W}(\matm{D}\vecm{q}_{k+1} - \vecm{z} + \bar{\vecm{u}}_k)}^2 \right), \\
    \bar{\vecm{u}}_{k+1} &= \bar{\vecm{u}}_k + \matm{D}{\vecm{q}}_{k+1} - \vecm{z}_{k+1},
\end{align}

\noindent where $\bar{\vecm{u}} = \matm{W}^{-1}\vecm{u}$. We point out that the matrix $\matm{M} + \rho h^2 \matm{D}^T\matm{W}^T\matm{W}\matm{D}$ in the $\vecm{q}$-update 
is constant across the entire simulation if the set of energy terms remains unchanged. Finally, note that $\rho$ can be absorbed into $\matm{W}$ via $\matm{W} \leftarrow 
\sqrt{\rho}\matm{W}$. Thus, $\rho$ is omitted in the following for convenience. 

Due to the separable structure of $\Psi_*$ and $\matm{W}$, the $\vecm{z}$- and $\bar{\vecm{u}}$-updates can be performed independently for each energy term. For each 
$\psi_j$, the corresponding subvectors $\vecm{z}_j$ and $\bar{\vecm{u}_j}$ can be updated via

\begin{align}
    \vecm{z}^{k+1}_j &= \argmin{\vecm{z}_j} \left( \psi_j(\vecm{z}_j) + \frac{1}{2}\norm{\matm{W}_j(\matm{D}_j\vecm{q}^{k+1} - \vecm{z}_j + \bar{\vecm{u}}^k_j)}^2 \right), 
    \label{eq:admm-z-update} \\
    \bar{\vecm{u}}^{k+1}_j &= \bar{\vecm{u}}^k_j + \matm{D}_j{\vecm{q}}^{k+1} - \vecm{z}^{k+1}_j. \label{eq:admm-u-update}
\end{align}

\noindent While the $\vecm{q}$- and $\bar{\vecm{u}}$-updates are trivial to perform, a solver for the optimization problem in Equation \ref{eq:admm-z-update} needs to be 
provided for each energy term $\psi_j$. This optimization problem is non-linear and does not have an analytical solution in the general case. Still, since the optimization 
problem is low dimensional for typical energy terms it can usually be solved at low computational cost via numerical optimization. An overview over implicit Euler 
integration via ADMM is given in Algorithm \ref{alg:admm}.

\begin{algorithm}[t]
\caption{Implicit Euler Integration via ADMM}\label{alg:admm}
\begin{algorithmic}
\Procedure{solveADMM}{$\vecm{q}_n$, $\vecm{v}_n$, $\vecm{f}_{\text{ext}}$, $h$}
\State $\tilde{\vecm{q}} = \vecm{q}_n + h\vecm{v}_n + h^2\matm{M}^{-1}\vecm{f}_{\text{ext}}$
\State $\vecm{q} = \tilde{\vecm{q}}$, $\bar{\vecm{u}} = \vecm{0}$
\For{all iterations}
\For{energy terms $j$}
\State Compute $\matm{D}_j\vecm{q} + \bar{\vecm{u}_j}$
\State Update $\vecm{z}_j$ using Equation \ref{eq:admm-z-update}
\State Update $\bar{\vecm{u}}_j$ using Equation \ref{eq:admm-u-update}
\EndFor
\State Update $\vecm{q}$ using Equation \ref{eq:admm-q-update}
\EndFor
\State \textbf{return with } $\matm{q}_{n+1} = \matm{q}, \matm{v}_{n+1} = (\matm{q}_{n+1} - \matm{q}_n) / h$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Hard Constraints}\label{ss:admm-hard-constraints}
Hard constraints can be incorporated into the ADMM solver in a straight-forward manner. Given a constraint manifold $\cman{C}_j$, we construct the energy term 

\begin{equation}\label{eq:admm-hard-constraint}
    \psi_j(\vecm{z}_j) = \delta_{\cman{C}_j}(\vecm{z}_j) = 
\begin{cases}
0,& \text{if } \matm{z}_j \text{ lies on the constraint manifold } \cman{C}_j \\
\infty,& \text{otherwise,}
\end{cases}
\end{equation}

\noindent where $\delta_{\cman{C}_j}$ is the indicator function for $\cman{C}_j$ (see Eq.\ \ref{eq:indicator-function}). Then, the $\vecm{z}_j$-update is as 
simple as projecting $\matm{D}_j\vecm{q}^{k+1} + \bar{\vecm{u}}^k_j$ onto the constraint manifold $\cman{C}_j$, i.e.\ 

\[
    \vecm{z}^{k+1}_j = \text{proj}_{\cman{C}_j}(\matm{D}_j \vecm{q}^{k+1} + \vecm{u}^{k}_j).
\]


\subsection{Properties of ADMM}\label{ss:admm-properties}
The ADMM solver approaches implicit Euler integration of the equations of motion through the lense of constrained optimization. In the following, we provide 
a detailed discussion of its properties.

\paragraph{Relation to PD.}
It can be shown that ADMM with weights $w_j = \sqrt{k_j}$ applied to PD energy potentials $\psi_j$ (see Eq.\ \ref{eq:pd-potentials}) with stiffness $k_j$
and constraint manifold $\cman{C}_j$ is equivalent to PD if $\cman{C}_j$ is affine. For a detailed proof of this statement, we refer the reader to the paper by 
Overby et al.\ \cite{overby2017}. Note that the proof does not apply for the strain material model (see Sec.\ \ref{ss:strain-material}), since the constraint 
manifolds used in the corresponding PD energy potentials are not affine (see Sec.\ \ref{ss:pd-deformable-bodies}). Still, the authors provide experimental data 
that shows that ADMM and PD exhibit almost identical behavior in this setting. As such, the ADMM solver can be interpreted as an 
extension of the PD solver that allows handling hard constraints and modelling arbitrary conservative energy terms.

\paragraph{Efficient Implementation.}
As pointed out in Section \ref{ss:admm-solver}, the system matrix in the $\vecm{q}$-update (see Eq.\ 
\ref{eq:admm-q-update}) is constant across the entire simulation if the constraint set remains unchanged. Thus, its matrix factorization can be reused during 
all $\vecm{q}$-updates, allowing for the LSE in Equation \ref{eq:admm-q-update} to be solved efficiently. Additionally, the $\vecm{z}_j$- and 
$\bar{\vecm{u}}$-updates can be computed independently for each energy term, enabling performance gains via parallelization. In many cases, the 
$\vecm{z}_j$-updates do not have an analytical solution and may need to be determined numerically. However, the corresponding optimization problem is low 
dimensional and can usually be computed at moderate computational cost. Either way, the impact of a single $\vecm{z}_j$-update on the runtime of the ADMM solver
becomes negligible with increasing hardware parallelism \cite{overby2017}.

\paragraph{Generality.}
The ADMM solver can be applied to hyperelastic energy terms $\psi_j(\vecm{q})$ for which the optimization problem in the $\vecm{z}_j$-update can be 
solved. While Overby et al.\ \cite{overby2017} provide a closed-form solution for the $\vecm{z}_j$-update for PD energy potentials, the $\vecm{z}_j$-update for 
(simplified) Neohookean energy terms needs to be computed numerically. The ADMM solver is capable of modelling arbitrary isotropic and anisotropic hyperelastic 
materials. This is in contrast to the QN solver, where anisotropy can only be modelled by adding an anisotropic stiffness term to Valanis-Landel energies 
\cite{liu2017}. Additionally, we demonstrate that the ADMM solver is compatible with energy terms that are not differentiable in Section \ref{ss:admm-hard-constraints}. 
This enables implementing hard constraints with the ADMM solver.

\paragraph{Convergence.}
ADMM is known to converge to the optimal solution of the constrained minimization problem in Equation \ref{eq:admm-general} with a convergence rate of $\mathcal{O}(1/n)$
for convex functions $f$ and $g$ under mild assumptions \cite{overby2017}. However, since the energy potentials of the material models used in physical simulations are 
generally not convex and $g(\vecm{z}) = \Psi_*(\vecm{z})$, these results may not apply directly to the ADMM solver. Thus, the performance of the ADMM solver is expected 
to deteriorate for energy terms that are characterized by large degrees of non-convexity. Note that even for convex $f, g$ the convergence rate of $\mathcal{O}(1/n)$ is 
much lower than the superlinear convergence rate observed for many Quasi-Newton methods \cite{nocedal2006}.

\paragraph{Weights.}
The convergence rate of the ADMM solver depends heavily on the choice of appropriate weights $w_j$ for energy terms $\psi_j$ \cite{overby2017}. To the best of our knowledge, 
no method for determining the choice of $w_j$ for reliably fast convergence is available. Overby et al.\ \cite{overby2017} provide experimental data that suggests 
that reducing $w_j$ can have beneficial effects on the convergence rate of ADMM, but that the algorithm may fail to converge if the weights are reduced too much. The 
first observation can be explained by taking a closer look at the $\vecm{z}_j$-update, which is restated here again for the convenience of the reader

\begin{equation}\label{eq:admm-z-update-copy}
    \vecm{z}^{k+1}_j &= \argmin{\vecm{z}_j} \left( \psi_j(\vecm{z}_j) + \frac{1}{2}\norm{\matm{W}_j(\matm{D}_j\vecm{q}^{k+1} - \vecm{z}_j + \bar{\vecm{u}}^k_j)}^2 \right).
\end{equation}

\noindent According to Overby et al.\ \cite{overby2017}, the $\vecm{z}_j$-update can be interpreted as a proximal operator for $\psi_j$ and intuitively amounts to 
minimizing $\psi_j$ with a quadratic penalty for moving away from $\matm{D}_j\vecm{x}^{n+1}+ \bar{\vecm{u}}^i_n$. This penalty increases with the weights that make 
up the entries of $\matm{W}_j$. Thus, reducing the weights allows making faster progress towards a solution that minimizes the constraint during 
the $\vecm{z}_j$-update. Keeping in mind that $\matm{D}_j\vecm{q}^* = \vecm{z}^*_j$ for all constraints $C_j$ once ADMM converges, it is natural to assume that using smaller
weights is particularly helpful when the converged positions $\vecm{q}^*$ reduce the total constraint energy $\Psi$ significantly in comparison to the initial positions 
$\vecm{q}_0$. However, if $\Psi(\vecm{q}^*) \approx \Psi(\vecm{q}_0)$, it might be counterproductive to take large steps towards a solution that minimizes the constraint 
energies during the $\vecm{z}_j$-update. This effect might also be responsible for ADMM's failure to converge if inappropriately small weights are used.

\paragraph{Changing Constraint Sets.}
Similar to the PD and QN solvers, the ADMM solver is not suited for handling frequently changing constraint sets. This is because efficient implementation of the ADMM 
solver exploits the fact that the system matrix of the $\vecm{q}$-update is constant, allowing for its factorization to be reused. However, Overby et al.\ 
\cite{overby2017} point out that collisions with static geometry can be handled efficiently using hard constraints. There, a single collision energy term 
$\psi_\text{coll}$ with reduction matrix $\matm{D}_\text{coll} = \matm{I} \in \mathbb{R}^{3m \times 3m}$ is added to the system. During the $\vecm{z}$-update, 
particles are simply projected to their closest non-penetrating positions.

\paragraph{Hard Constraints.}
In Section \ref{ss:admm-hard-constraints}, we demonstrated how to model hard constraints using the ADMM solver by introducing suitable energy terms $\psi_j$.
\noindent Overby et al.\ \cite{overby2017} point out that while the $\vecm{z}_j$ satisfy their constraints at all times, the same may not be true for the particle 
positions $\vecm{q}$ before convergence is achieved. This is particularly important in the context of real-time applications, where the number of solver iterations 
is often capped to ensure that the physics simulations fit into the time budget of a single frame.
