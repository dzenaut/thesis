\documentclass{article}
    % General document formatting
    \usepackage[margin=0.7in]{geometry}
    \usepackage[parfill]{parskip}
    \usepackage[utf8]{inputenc}

    % Images
    \usepackage{graphicx}

    % Colors
    \usepackage{xcolor}
    
    % Related to math
    \usepackage{amsmath,amssymb,amsfonts,amsthm,bm, mathtools}

    \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\subsection*{Dynamic Simulation}
\subsubsection*{TODOS}
\begin{itemize}
    \item What is dynamic simulation?
    \item What is the standard approach?
    \item What are the challenges?
    \item Enter constraints!
\end{itemize}

Simulating effects such as incompressibility, inextensibility and joints between articulated rigid bodies 
in elasticity-based simulations can be achieved by using high stiffness values. High stiffness values lead to 
large forces which in turn cause numerical issues in the solver. 

We demonstrate these issues based on the example of maintaining a desired distance between two points using a stiff 
spring \cite{tournier2015}. Let $\bm{x_1, x_2}$ be the positions, $\bm{v_1, v_2}$ the velocities and $\bm{a_1, a_2}$
be the accelerations of the two particles. Let $\overline{l}$ be the rest length and $l = \lVert \bm{x_1} - \bm{x_2} \rVert$ 
be the current length of the spring with stiffness $k$. It can be shown that the force that the spring applies at each particle
is equal to $\bm{f_1} = -\bm{f_2} = \lambda\bm{u}$, where $\bm{u} = (\bm{x_1} - \bm{x_2}) / l$
and $\lambda = -\frac{\delta V}{\delta l} = k(\overline{l} - l)$. 

Once the forces, accelerations, velocities and positions are combined into vectors $\bm{f}, \bm{a}, \bm{v}, \bm{x}$, 
respectively, the motions of the system can be modeled via Newton's Ordinary Differential Equation (ODE) $\bm{f} = \bm{Ma}$,
where $\bm{M}$ is a $n_d \times n_d$ diagonal matrix and $n_d$ is the total number of independent degrees of freedom for the 
particles.

This system can be integrated via the symplectic Euler method as follows:

\begin{align*}
    \bm{v_{n+1}} &= \bm{v_n} + h\bm{a_n} \\
    \bm{x_{n+1}} &= \bm{x_n} + h\bm{v_{n+1}}
\end{align*}

As the stiffness $k$ of the spring increases, so does the magnitude of the acceleration $\bm{a}$. Consequently, the integration
diverges unless the timestep is prohibitively small. The stability issues are often addressed by switching to an implicit 
integration scheme, such as the backward Euler method \cite{baraff1998}. Replacing current accelerations with future accelerations
requires the solution of the following linear system of equations (LSE):

\[
    (\bm{M} - h^2\bm{K})\bm{v_{n+1}} = \bm{p} + h\bm{f}
\]

where $\bm{p} = \bm{Mv_n}$ is the momentum, and $\bm{K} = \frac{\delta \bm{f}}{\delta \bm{x}}$ is the stiffness matrix. Note that 
$\bm{K}$ is typically non-singular since elastic forces are invariant under rigid body transforms. When using large stiffness 
$k$ for springs, the entries of $\bm{K}$ are large (due to large restorative forces for stiff springs) and dominate the entries 
of the system matrix $\bm{H} = \bm{M} - h^2\bm{K}$. In these cases, $\bm{H}$ will be almost non-singular as well, leading to 
numerical issues and poor convergence for many solvers. Additionally, implicit integration introduces noticable numerical damping
\cite{servin2006}.

\textcolor{blue}{This system results from performing the implicit integration and solving the non-linear system via linearization
using the Taylor expansion. Positions can be expressed in terms of velocities and eliminated from the system.}

\subsection*{Penalty Forces}
In the example above, the energy was derived from Hooke's Law for springs. However, it is also possible to derive energies from 
geometric displacement functions $\bm{\phi(x)}$ which vanish in the rest configuration. From the displacement functions, quadratic 
potential energies of the form $U(\bm{x}) = \Sigma_i (k / 2) \bm{\phi^2(x)}$, where $k$ is a positive stiffness parameter, are 
constructed \cite{terz1987}. The potential energy $U(\bm{x})$ is zero if the displacement function is satisfied, and greater than 
zero otherwise. The resulting forces are called penalty forces.
Using the geometric displacement function $\bm{\phi_{\text{spring}}(x)} = (\lVert \bm{x_i} - \bm{x_j} \rVert) - l$ with $k_{\text{spring}}$ 
recovers the behavior of a spring with stiffness $k_{\text{spring}}$. Its displacement function $\bm{\phi_{\text{spring}}(x)}$ is 
satisfied when the distance of the particles $\bm{x_i}, \bm{x_j}$ is equal to a desired rest length $l$. By constructing different 
geometric displacement functions, various properties such as the bending angle between triangles and in-plane shearing of triangles 
can be controlled via the corresponding quadratic energy potentials \cite{baraff1998}. Geometric displacement functions with the desired effect are 
often intuitive and simple to define. However, as the corresponding energy potentials are not physically derived, choosing stiffness 
parameters that correspond to measurable physical properties of the simulated material and orchestrating multiple constraints becomes 
challenging \cite{servin2006, nealen2006}. Additionally, the generated penalty forces do not converge in the limit of infinite stiffess, leading
to oscillations unless the timestep is reduced significantly \cite{rubin1957}.

\textcolor{blue}{Maybe explain the challenges with penalty forces a bit better! Also read \cite{terz1987, nealen2006, rubin1957}. 
I just skimmed over \cite{terz1987} for now, but want to make sure that I am citing this correctly. The term penalty forces is not used
in the paper, I am just following the trail from \cite{servin2006}. \cite{nealen2006} is a review that might be intersting to read.
\cite{rubin1957} would be really interesting to read for once, just to understand why strong penalty forces oscillate. Is this a
general problem with penalty forces, or is it an issue with the solver?}

\subsection*{Mass Modification}
The motion of a particle can be influenced by modifying the inverse mass matrix $\bm{M^{-1}}$ of the system. 
For a single particle $\bm{x_i}$, it is $\ddot{\bm{x_i}} = \bm{M_i^{-1} f}$, where $\bm{M_i^{-1}}$ is the inverse mass matrix for 
particle $i$. If, for example, the first diagonal entry of $\bm{M_i^{-1}}$ is zero, no acceleration in the x-direction is possible. 
It is possible to construct modified inverse mass matrices $\bm{W}$ such that the accelerations in the three axes of an arbitraty 
orthogonal coordinate system can be restricted. The modified inverse mass matrices $\bm{W}$ can be used in the LSE that results 
from the implicit integration above. By adding simple velocity and position terms to the system equations the magnitude of the 
change in velocity in each direction and even the exact position of each constrained particle can be controlled. This approach is
called mass modification \cite{baraff1998}. In \cite{baraff1998}, the authors use mass modification to model collision constraints 
between objects and cloth and other user defined constraints. 

Since the velocity and position of each constrained particle is controlled via a single velocity and position term, multiple 
constraints that affect the same particle have to be handled together. This can lead to constraints which affect arbitrarily many
particles. For that reason, self-collisions of cloth are not handled via mass modification in \cite{baraff1998}. Instead, penalty
forces are used. Additionally, accurately constraining particle positions is only possible for particles whose velocity is 
constrained as well. The resulting system is unbanded, but sparse, and is solved using a modified version of the conjugate gradient
(CG) method.

\subsection*{Constraint-based Dynamics}

\subsubsection*{Hard Constraints}
The problem of maintaining hard distance constraints between particles can be formulated as a Differential Algebraic Equation (DAE)
\cite{ascher1995, baraff1996}. In this framework, the standard ODE $\bm{f} = \bm{Ma}$ is handled together with algebraic equations 
that model the constraints on the system. Distance constraints are typically implemented using holonomic constraints of the form 
$\bm{\phi(x)} = 0$. Note that the distance constraint $\bm{\phi}(x)$ is formulated in terms of the particle positions, whereas the ODE
works on accelerations or velocities. Consequently, the constraints need to be differentiated once or twice so that they can be 
combined with the ODE in terms of velocties or accelerations, respectively. \textcolor{blue}{In xPBD, we go the other way! The ODE
is tanslated so that it is in terms of positions, so that it can be handled together with the constraints. Is there a reason nobody bothered to
do this before? What are the challenges here? Is this exactly what xPBD is? Is there a way to view the simplifications made in 
terms of the other frameworks?}. Using $\bm{J} = \frac{\delta \bm{\phi}}{\delta \bm{x}}$, where $\bm{J}$ is a $n_c \times n_d$ matrix
and $n_c$ is the number of scalar constraints, this leads to the following constraint formulations:

\begin{align*}
    \bm{Jv} &= 0 \\
    \bm{Ja} &= \bm{c(v)}
\end{align*}

for some $\bm{c(v)}$. \textcolor{red}{If you check \cite{ascher1995}, see that $c(v)$ also depends on the positions $q$. That should be indicated!} 
Additionally, constraint forces are required in order to link the algebraic constraint equations with the ODE describing the motion of 
the system. It can be shown that the constraint forces $\bm{f_c}$ applied to the particles have to be in the following form in order to 
avoid adding linear and angular momentum to the system \cite{baraff1996}:

\[
    \bm{f_c} = \bm{J^T \lambda}
\]

where the $\lambda$ are the Lagrange multipliers of the constraints. With external forces $\bm{f_e}$, the DAE can now be expressed as follows 
\cite{ascher1995}:

\[
    \begin{pmatrix}
        \bm{M} & \bm{-J^T} \\
        \bm{J} & 0
    \end{pmatrix}
    \begin{pmatrix}
        \bm{a} \\
        \bm{\lambda}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \bm{f_e} \\
        \bm{c(v)}
    \end{pmatrix}
\]

Note that the lower block-row of the system drives towards accelerations that satisfy the constraints imposed by $\bm{\phi(x)}$ (or, striclty 
speaking, the differentiations thereof) exactly. This is indicated by the lower-right zero block in the system matrix in either formulation. 
Thus, the system does not have a solution if constraints are contradictory. \textcolor{blue}{Aren't $\dot{q} = v$ and $\dot{v} = a$ 
also part of the differential equation? Because $c(v)$ and $f_e$ also depend on $q$!}

In \cite{ascher1995}, the DAE is approached by eliminating the $\lambda$ from the system entirely and constructing an ODE in terms of positions
and velocities. In \cite{tournier2015}, the authors suggest applying implicit integration schemes to the system directly by constructing the 
following Karush-Kuhn-Tucker (KKT) equation system:

\[
\begin{pmatrix}
    \bm{M} & \bm{-J^T} \\
    \bm{J} & 0
\end{pmatrix}
\begin{pmatrix}
    \bm{v_{n+1}} \\
    \bm{\mu_{n+1}}
\end{pmatrix}
=
\begin{pmatrix}
    \bm{p} + h\bm{f_e} \\
    0
\end{pmatrix}
\]

Here, the external forces $\bm{f_e}$ and the constraint gradients $\bm{J}$ are considered constant across the timestep and $\bm{J(x_{n+1})}$ 
is not approximated using the Taylor expansion like it is in \cite{baraff1998}. If internal forces are taken into account, the upper-left 
matrix $\bm{M}$ is replaced by the matrix $\bm{H}$ from above.

\textcolor{blue}{Reverse-engineering how the authors arrived at this system is quite enlightening. Start out from the equations of motion 
\cite{ascher1995}
    \begin{align*}
        \dot{\bm{v}} &= \bm{M^{-1}(f - J^T) \lambda}
    \end{align*}
    and perform implicit integration:
    \begin{align*}
        \bm{v_{n+1}} &= \bm{v_n} + h\bm{M^{-1}(f_e(x_{n+1})} - \bm{J^T(x_{n+1})\lambda(x_{n+1}))} \\
        \bm{Mv_{n+1}} &= \bm{p} + h\bm{f_e(x_{n+1})} - h\bm{J^T(x_{n+1})\lambda(x_{n+1})} \\ 
        \bm{Mv_{n+1}} + h\bm{J^T(x_{n+1})\lambda(x_{n+1})} &= \bm{p} + h\bm{f_e(x_{n+1})} \\ 
        \bm{Mv_{n+1}} + \bm{J^T(x_{n+1}) \mu(x_{n+1})} &= \bm{p} + h\bm{f_e(x_{n+1})}
    \end{align*}
}
\textcolor{red}{If we assume that $\bm{f_e}$ and the constraint gradients $\bm{J}$ are constant across the time step, we arrive at the 
formulation from the paper. For the external forces, which are usually only comprised of gravitational forces, this is not a big 
deal. For the constraint gradients, I am not sure what the ramifications are. In \cite{baraff1998}, the Taylor expansion is performed
which requires the compution of second derivatives over the constraint functions. This is not happening here at all! Is this what 
authors mean when they say that the constraints are effectively linearized during one solve, e.g. second page 
of \cite{mueller2020}? Technically, speaking, even if the Taylor expansion is performed, the constraints are linearized, if I 
understand correctly.}

Note that the system matrix is sparse, which can be exploited by sparse-matrix solvers in order to solve the system efficiently
\cite{baraff1996}. Alternatively, the Schur complement can be constructed since the mass matrix in the upper left block is invertible.
This leads to a smaller, albeit less sparse system \cite{tournier2015}:

\[
    \bm{JM^{-1}J^T \mu} = \bm{-JM^{-1}(p + } h \bm{f_e)}
\]

If the constraints are not redundant, $\bm{JM^{-1}J^T}$ is non-singular and symmetric positive definite \cite{baraff1996}, which are desirable
properties for many solvers. According to \cite{servin2006}, the common approaches for linearizing the constraint forces and stabilizing the constraints 
$\bm{\phi(x)} = 0$ are notoriously unstable. Additionally, instabilities in the traverse direction of the constraints occur when the tensile force with 
respect to particle masses is large when using hard constraints \cite{tournier2015}.

\subsubsection*{Compliant Constraints}
By combining ideas from hard constraints and penalty forces, it is possible to formulate the system matrix for hard constraints such that
constraints do not have to be enforced exactly. In this approach, called compliant constraints, the constraints are combined with the the ODE
$\bm{f} = \bm{Ma}$ in a way that allows relaxation of constraints in a physically meaningful manner \cite{servin2006}. The key insight is that
constraints of the form $\bm{\phi(x)}$ are the physical limit of strong potential forces of the form $\frac{k}{2}\bm{\phi^2(x)}$ with high 
stiffness values $k$. However, using large, but finite, stiffness values has adverse affects on the numerical properties of the system matrix. 
Thus, the equations of motion are rewritten in terms of the inverse stiffness. The potential energy for the constraint $\bm{\phi}$ is then 
defined as:

\[
    U(\bm{x}) = \frac{1}{2}\bm{\phi^T(x)} \alpha^{-1}\bm{\phi(x)}
\]

where $\alpha$ is a symmetric, positive definite matrix of dimension $n_c \times n_c$. The correspondence to the penalty terms above is the 
case where $\alpha^{-1}$ is a diagonal matrix with diagonal entries $\frac{1}{k_i}$ for the stiffness $k_i$ of constraint $\bm{\phi(x)}$. The
resulting forces $\bm{f_c} = \delta U / \delta \bm{x} = -J^T\alpha^{-1} \bm{\phi}$. In order to replace the large parameters $\alpha^{-1}$ with
the small $\alpha$ in the equations of motion, artificial variables $\lambda = -\alpha^{-1}\bm{\phi}$ are introduced, yielding 
$\bm{f_c} = J^T\lambda$.

This leads to the following DAE:

\begin{align*}
    \bm{\dot{x}} &= \bm{v} \\
    \bm{M\dot{v}} &= \bm{f_{e} + J^T\lambda} \\
    \alpha\lambda(\bm{x}, t) &= -\bm{\phi}(\bm{x}, t)
\end{align*}

Note, that in the limit of infinite stiffness, the formulation from hard constraints is recovered. By performing backwards differentiation and 
assuming that the constraint gradients are constant across the timestep, it is:

\[
    \bm{\alpha\lambda_{n+1}} = \bm{C}\frac{\bm{\mu_{n+1}}}{h} = -\bm{\phi_{n+1}} \approx -\bm{\phi} - h\bm{Jv_{n+1}}
\]

leading to the following LSE \cite{tournier2015}:

\[
\begin{pmatrix}
    \bm{M} & \bm{-J^T} \\
    \bm{J} & \frac{1}{h^2}\bm{\alpha}
\end{pmatrix}
\begin{pmatrix}
    \bm{v_{n+1}} \\
    \bm{\mu_{n+1}}
\end{pmatrix}
=
\begin{pmatrix}
    \bm{p} + h\bm{f_e} \\
    - \frac{1}{h}\bm{\phi}
\end{pmatrix}
\]

\textcolor{red}{Regarding the backwards differentiation above, this does not perform the Taylor approximation again. It should be something
like:
\begin{align*}
    \phi_+ \approx \phi + h\dot{\phi_+} &= \phi + hJ_+v_+ \\
                                        &\approx \phi + h(J + h\dot{J})v_+ \\
                                        &= \phi + h(J + h\frac{\delta J}{\delta x}\frac{\delta x}{\delta t})v_+ \\
                                        &= \phi + h(J + h\frac{\delta J}{\delta x}v)v_+
\end{align*}
Now, we need second derivatives of the constraints. This can be seen in \cite{baraff1998} and is also mentioned in \cite{servin2006}.
}

This formulation comes with a couple of advantages. Firstly, relaxing the constraints by keeping a finite but large penalty parameter helps
counteracting numerical problems in the presence of over defined or degenerate constraints. Secondly, in comparison to the system from hard
constraints, introducing $\alpha$ in the lower right block of the system matrix makes the system matrix strongly positive definite, which 
is beneficial for many solvers. Lastly, in comparison to penalty forces, entries of large magnitudes in the system matrix due to high 
stiffness terms are exchanged for small entries in terms of inverse stiffness, which improves the condition number of the matrix. 

\textcolor{red}{
    All these concepts from numerics are a bit unclear to me. I might have to go back to some textbook and do some reading to improve my understanding. 
    I might have to go back to some textbook and do some reading to improve my understanding. Not sure the last part is entirely true.}

\textcolor{red}{
    In \cite{servin2006}, a solver based on symplectic Euler which does not require second derivatives is derived. I do not understand some of the
    estimations made in that derivation. In particular the mean of a function $f$ over and interval $(a, b)$ is defined as $\frac{1}{b-a}
    \int_a^b f(x) dx$, so what they are saying does not make a lot of sense.}


\subsection*{Projective Dynamics}

In the approaches to physical simulations via implicit time integration that we have encountered so far, a new linear system needs to be solved
at every timestep. For large simulations, tackling a new, different linear system of equations every time step can quickly become prohibitively 
expensive. At each timestep, new factorizations for the system matrix need to be computed. In PBD, this issue is dealt with by using an iterative 
solver. In Projective Dynamics (PD), a different approach is used. Energy potentials are restricted to a specific structure which allow for 
efficient implicit time integration via alternating steps of local and global optimization. The local optimization steps are comprised of 
per-constraint projections of particle positions onto constraint manifolds. The global optimization step combines the results from the individual 
local projection steps while taking into consideration global effects including inertia and external forces. This is achieved by solving a 
linear system of equations whose system matrix is constant across timesteps. Since the local steps can be carried out in parallel and the 
factorization for the system matrix of the global step can be precomputed and reused, physical simulations that are restricted to energy potentials 
from the PD framework can be solved efficienty and robustly.

\subsubsection*{Energy Potentials}

Let the positions of $m$ particles in a mesh be stored in a matrix $\bm{q} \in \mathbb{R}^{m \times 3}$ with deformation gradient $\bm{F} 
\coloneqq \bm{F(\bm{q})} \in \mathbb{R}^{3 \times 3}$. Then, energy potentials of the general form $\psi(\bm{E}(\bm{F}))$, where 
$\bm{E}(\bm{F})$ is a strain measure that depends on the deformation gradient of a discrete element, are frequently used in nonlinear 
continuum mechanics. If $\bm{E}$ is Green's strain measure $\bm{E_{\text{Green}}}$ defined by 

\[
\bm{E_{\text{Green}}}(\bm{F}) = \frac{1}{2}(\bm{F^TF} - \bm{I})
\]

then $\bm{E_{\text{Green}}}(\bm{F}) = 0$ is equivalent to $\bm{F^TF} = \bm{I}$. Thus, $\bm{E_{\text{Green}}}(\bm{F}) = 0$ defines a constraint
manifold that accepts deformations whose deformation gradients $\bm{F}$ are rotation matrices. These deformations are exactly the rigid-body 
transforms, i.e. transforms that alter the body's position and orientation but keep the body's volume undeformed. Assuming that $\psi(\bm{0}) 
= \rho$ for some $\rho \in \mathbb{R}$ and that $\psi$ reaches its minimum at the undeformed configuration, then 

\[
    d(\bm{E_{\text{Green}}}(\bm{F})) = \psi(\bm{E_{\text{Green}}}(\bm{F})) - \rho
\]

can be considered a distance measure of how far the configuration is from the constraint manifold defined by the undeformed configurations. 

The energy potentials in PD are designed to fit into this framework: Energy potentials are defined by a constraint manifold $\bm{C}$ -- which 
can be different from $\bm{E_{\text{Green}}(\bm{F})} = 0$ -- and a distance measure $d$ of the body's current configuration to that constraint 
manifold. Formally, this leads to energy potentials which of the 
following form:

\[
    \psi({\bm{q}}) = \min_{\bm{p}} d(\bm{q}, \bm{p}) + \delta_{\bm{C}}(\bm{p}).
\]

Here, $\bm{p} \in \mathbb{R}^{r \times 3}, r \in \mathbb{N}$ are auxiliary projection variables and $\delta_{\bm{C}}(\bm{p})$ is an indicator function with 

\[
    \delta_{\bm{C}}(\bm{p})= 
\begin{cases}
0,& \text{if } \bm{p} \text{ lies on the constraint manifold } \bm{C}\\
\infty,& \text{otherwise.}
\end{cases}
\]

Define $\bm{p_{\bm{q}}}$ such that $\psi({\bm{q}}) = d(\bm{q}, \bm{p_{\bm{q}}}) + \delta_{\bm{C}}(\bm{p_{\bm{q}}})$. Then obviously 
$\delta_{\bm{C}}(\bm{p_{\bm{q}}}) = 0$, meaning that $\bm{p_{\bm{q}}}$ lies on $\bm{C}$. Together, $\bm{p_{\bm{q}}}$ is the 
configuration on the constraint manifold $\bm{C}$ with minimal distance $d(\bm{q}, \bm{p_{\bm{q}}})$ to current configuration $\bm{q}$.
Consequently, $\psi(\bm{q})$ measures the distance of $\bm{q}$ to the constraint manifold $\bm{C}$.

The authors claim that since the constraint manifolds already capture nonlinearities the need for complicated distance functions $d$
can be relaxed while still achieving visually plausible simulations. In PD, distance measures $d$ are restricted to quadratic functions 
of the form

% TODO: Mention that q and p have three columns. Thus, no x-y-z interations are captured in distance functions of the form given below

\[
    d(\bm{q}) = \frac{w}{2} \norm{\bm{Gq} - \bm{p}}^2_F,
\]

where $\bm{G} \in \mathbb{R}^{r \times m}$ for some $r \in \mathbb{N}$.

Note that since $\bm{p}, \bm{q} \in \mathbb{R}^{m \times 3}$, the distance measure $d$ has no dependencies between $x$-, $y$- and 
$z$-coordinates. This detail demonstrates that restricting to PD energy potentials comes at the cost of generality: Many arbitrary 
nonlinear elastic potentials, particulary those that have dependencies between $x$-, $y$- and $z$-coordinates, cannot be expressed 
in terms of PD elastic potentials. However, their structure enables an efficient algorithm for implicit integration, as discussed in
the \textcolor{red}{next section}.

In summary, PD energy potentials have the following form:

\[
    \psi({\bm{q}}) = \min_{\bm{p}} \frac{w}{2} \norm{\bm{Gq} - \bm{p}}^2_F + \delta_{\bm{C}}(\bm{p}).
\]

\subsubsection*{Equations of Motion}
\textcolor{red}{Should there be a rough general introduction to implicit integration? Or is it enough to show the resulting equations
in the context of the ODE with the equations of motion?}

\textcolor{red}{For now, this only has the information required for PD. Check whether more information is required for (x)PBD, ADMM, QN.}

\textcolor{red}{Maybe mention other integration schemes than the implicit Euler integration.}

The motion of a spatially discretized system with $m$ particles evolving in time according to Newton's laws of motion can be modeled via 
the following ordinary differential equation (ODE), which will be referred to as Newton's ODE \cite{bouaziz2014, macklin2016, bender2017}:

\begin{align*}
    \bm{q}(t)\prime &= \bm{v}(t) \\
    \bm{v}(t)\prime &= \bm{M}^{-1}\bm{f}(\bm{q}(t), \bm{v}(t))
\end{align*}

where $\bm{q}(t)$, $\bm{v}(t)$, $\bm{f}(\bm{q}(t), \bm{v}(t))$ are the particle positions, particle velocities and forces acting on each particle
at time $t$, respectively, and $\bm{M}$ is a diagonal matrix with the particle masses as diagonal entries. Depending on the context, either 
$\bm{q}(t), \bm{v}(t), \bm{f}(\bm{q}(t), \bm{v}(t)) \in \mathbb{R}^{m \times 3}$ and $\bm{M} \in \mathbb{R}^{m \times m}$ or $\bm{q}(t), 
\bm{v}(t), \bm{f}(\bm{q}(t), \bm{v}(t)) \in \mathbb{R}^{3m}$ and $\bm{M} \in \mathbb{R}^{3m \times 3m}$. $\bm{q}(t)\prime$ and $\bm{v}(t)\prime$ are short for $\bm{D_tq}(t)$ and
$\bm{D_tv}(t)$, respectively. From now on, we write $\bm{q}$ instead of $\bm{q}(t)$ for time-dependent quantities for the sake of brevity.

\subsubsection*{Integration of Newton's ODE}
For general nonlinear forces, analytical solutions to Newton's ODE are usually not available. Thus, the ODE needs to be solved numerically. 
One popular integration scheme for tackling Newton's ODE is implicit Euler integration \cite{bouaziz2014, macklin2016, bender2017}. Here, 
the positions and velocities are computed at discrete timesteps via the following update formulas

\begin{align*}
    \bm{q}_{n+1} &= \bm{q}_n + h\bm{v}_{n+1}\\
    \bm{v}_{n+1} &= \bm{v}_n + h\bm{M}^{-1}\bm{f}(\bm{q}_{n+1}, \bm{p}_{n+1})
\end{align*}

for some timestep $h$. Note how $\bm{q}_{n+1}$ and $\bm{v}_{n+1}$ appear on both sides of the equations. Consequently, performing implicit 
Euler integration includes solving a set of nonlinear algebraic equations. It can be shown that implicit Euler integration is unconditionally 
stable and first-order accurate \cite{chapra2005}. However, implicit Euler integration is also known to exhibit numerical damping 
\cite{servin2006, bender2017, macklin2019}.

By rewriting the \textcolor{red}{first equation} as 

\[
    \bm{v}_{n+1} = \frac{1}{h}(\bm{q}_{n+1} - \bm{q}_n)
\]

and substituting into the \textcolor{red}{second equation} the following equation can be derived

\[
    \bm{M}(\bm{q}_{n+1} - \bm{q}_n - h\bm{v}_n) = h^2(\bm{f}(\bm{q}_{n+1}, \bm{v}_{n+1})).
\]

We separate forces $\bm{f}(\bm{q}, \bm{v})$ into internal forces $\bm{f}_{\text{int}}(\bm{q}, \bm{p}) = \sum_i \bm{f}^i_{\text{int}}
(\bm{q}, \bm{p})$ and external forces $\bm{f}_{\text{ext}}(\bm{q}, \bm{p}) = \sum_i \bm{f}^i_{\text{ext}}(\bm{q}, \bm{p})$. 
We consider all external forces to be constant. Internal forces are conservative and defined in terms of scalar potential energy functions 
$\psi_i$ via $\bm{f}^i_{\text{int}}(\bm{q}) = -\nabla \psi_i(\bm{q})$. Together, we have $\bm{f}(\bm{q}, \bm{v}) = \bm{f} (\bm{q}) 
= \bm{f}_{\text{int}} (\bm{q}) + \bm{f}_{\text{ext}} = \sum_i -\nabla \psi_i(\bm{q}) + \bm{f}_{\text{ext}}$. Plugging into the 
\textcolor{red}{equation above}, it is

\[
    \bm{M}(\bm{q}_{n+1} - \bm{q}_n - h\bm{v}_n) = h^2(\bm{f_\text{ext}} - \sum_i \nabla \psi_i(\bm{q})).
\]

By computing first-order optimality conditions, it is easily verified that the above system of equations is equivalent to the optimization 
problem

\[
    \min_{\bm{q}_{n+1}} \frac{1}{2h^2} \norm{\bm{q}_{n+1} - \bm{s}_n}^2_F + \sum_i \psi_i(\bm{q}_{n+1}).
\]

where $\bm{s}_n = \bm{q}_n + h\bm{v}_n + h^2\bm{M}^{-1}\bm{f}_{\text{ext}}$. This minimization problem whose solution corresponds to the next
iteration of the state of the implicit Euler integration is called the variational form of implicit Euler integration \cite{bouaziz2014}. 
The first and second term of the objective function are called the momentum potential and the elastic potential, respectively. Thus, the 
minimization problem requires that the solution minimizes the elastic deformation as best as possible while ensuring that the solution is 
close to following its momentum plus external forces. The weighting between the momentum potential and the elastic potential depends on the 
particle masses $\bm{M}$, the timestep $h$ and the material stiffness of the elastic potentials $\psi_i$. According to Noether's theorem, 
the solution preserves linear and angular momentum as long as the elastic potentials are rigid motion invariant.

\textcolor{red}{Add some words about how it is often favorable to use the variational formulation because solving an optimization problem is
often easier than just solving a system of equations. That is because one can be guided by the objective function.}



\begin{itemize}
    \item Restrict energy potientials to a specific structure. Convex quadratic distance measure from a constraint. The constraints are general
        nonlinear functions that express the desired state of an element.
    \item This structure trades general material nonlinearity (some energy potentials cannot be recreated using with this structure) against 
        efficiency and robustness.
    \item Structure of these constraint-based energy potentials allows efficient local/global optimization (block coordinate descent).
    \item Energy decreases weakly in every iteration without any specific precautions.
    \item For a fixed set of constraints, the linear system of the global step can be pre-factored.
    \item The local steps consists of small independent optimization problems, which can be all executed in parallel.
    \item Robust and efficient, often significantly outperforming classical Newton methods.
    \item Projective dynamics is an extension of \cite{liu2013} from mass-sprint systems to exmploying projection onto constraint sets to 
        simulate general nodal dynamical systems.
    \item Constraint projection is performed globally here, while it is performed locally in (x)PBD
        \begin{itemize}
            \item Understand exactly where and how the projection is achieved in (x)PBD
        \end{itemize}

    \item \textcolor{red}{The global solve (10) has a Hessian matrix according to \cite{bouaziz2014}. Look at the minimization problem with
        fixed projetion variables again }

    \item \cite{bouaziz2014} show some interesting experiments in their results. Additionally, the listed weaknesses of projective dynamics are 
        pretty interesting.
\end{itemize}

% Bibliography ------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{bibliography/references}

\end{document}
