\documentclass{article}
    % General document formatting
    \usepackage[margin=0.7in]{geometry}
    \usepackage[parfill]{parskip}
    \usepackage[utf8]{inputenc}

    % Images
    \usepackage{graphicx}

    % Colors
    \usepackage{xcolor}
    
    % Related to math
    \usepackage{amsmath,amssymb,amsfonts,amsthm,bm, mathtools}

\begin{document}

\subsection*{Dynamic Simulation}
\subsubsection*{TODOS}
\begin{itemize}
    \item What is dynamic simulation?
    \item What is the standard approach?
    \item What are the challenges?
    \item Enter constraints!
\end{itemize}

Simulating effects such as incompressibility, inextensibility and joints between atriculated rigid bodies 
can be achieved in elasticity-based simulations by using high stiffness values. High stiffness values lead to 
large forces which in turn cause numerical issues in the solver. 

We demonstrate these issues based on the example of maintaining a desired distance between two points using a stiff 
spring \cite{tournier2015}. Let $\bm{x_1, x_2}$ be the positions, $\bm{v_1, v_2}$ the velocities and $\bm{a_1, a_2}$
be the accelerations of the two particles. Let $\overline{l}$ be the rest length and $l = \lVert \bm{x_1} - \bm{x_2} \rVert$ 
be the current length of the spring with stiffness $k$. It can be shown that the force that the spring applies at each particle
is equal to $\bm{f_1} = -\bm{f_2} = \lambda\bm{u}$, where $\bm{u} = (\bm{x_1} - \bm{x_2}) / l$
and $\lambda = -\frac{\delta V}{\delta l} = k(\overline{l} - l)$. 

Once the forces, accelerations, velocities and positions are combined into vectors $\bm{f}, \bm{a}, \bm{v}, \bm{x}$, 
respectively, the motions of the system can be modeled via Newton's Ordinary Differential Equation (ODE) $\bm{f} = \bm{Ma}$,
where $\bm{M}$ is a $n_d \times n_d$ diagonal matrix and $n_d$ is the total number of independent degrees of freedom for the 
particles.

This system can be integrated via the symplectic Euler method as follows:

\begin{align*}
    \bm{v_{n+1}} &= \bm{v_n} + h\bm{a_n} \\
    \bm{x_{n+1}} &= \bm{x_n} + h\bm{v_{n+1}}
\end{align*}

As the stiffness $k$ of the spring increases, so does the magnitude of the acceleration $\bm{a}$. Consequently, the integration
diverges unless the timestep is prohibitively small. The stability issues are often addressed by switching to an implicit 
integration scheme, such as the backward Euler method \cite{baraff1998}. Replacing current accelerations with future accelerations
requires the solution of the following linear system of equations (LSE):

\[
    (\bm{M} - h^2\bm{K})\bm{v_{n+1}} = \bm{p} + h\bm{f}
\]

where $\bm{p} = \bm{Mv_n}$ is the momentum, and $\bm{K} = \frac{\delta \bm{f}}{\delta \bm{x}}$ is the stiffness matrix. Note that 
$\bm{K}$ is typically non-singular since elastic forces are invariant under rigid body transforms. When using large stiffness 
$k$ for springs, the entries of $\bm{K}$ are large (due to large restorative forces for stiff springs) and dominate the entries 
of the system matrix $\bm{H} = \bm{M} - h^2\bm{K}$. In these cases, $\bm{H}$ will be almost non-singular as well, leading to 
numerical issues and poor convergence for many solvers. Additionally, implicit integration introduces noticable numerical damping
\cite{servin2006}.

\textcolor{blue}{This system results from performing the implicit integration and solving the non-linear system via linearization
using the Taylor expansion. Positions can be expressed in terms of velocities and eliminated from the system.}

\subsection*{Penalty Forces}
In the example above, the energy was derived from Hooke's Law for springs. However, it is also possible to derive energies from 
geometric displacement functions $\bm{\phi(x)}$ which vanish in the rest configuration. From the displacement functions, quadratic 
potential energies of the form $U(\bm{x}) = \Sigma_i (k / 2) \bm{\phi^2(x)}$, where $k$ is a positive stiffness parameter, are 
constructed \cite{terz1987}. The potential energy $U(\bm{x})$ is zero if the displacement function is satisfied, and greater than 
zero otherwise. The resulting forces are called penalty forces.
Using the geometric displacement function $\bm{\phi_{\text{spring}}(x)} = (\lVert \bm{x_i} - \bm{x_j} \rVert) - l$ with $k_{\text{spring}}$ 
recovers the behavior of a spring with stiffness $k_{\text{spring}}$. Its displacement function $\bm{\phi_{\text{spring}}(x)}$ is 
satisfied when the distance of the particles $\bm{x_i}, \bm{x_j}$ is equal to a desired rest length $l$. By constructing different 
geometric displacement functions, various properties such as the bending angle between triangles and in-plane shearing of triangles 
can be controlled via the corresponding quadratic energy potentials \cite{baraff1998}. Geometric displacement functions with the desired effect are 
often intuitive and simple to define. However, as the corresponding energy potentials are not physically derived, choosing stiffness 
parameters that correspond to measurable physical properties of the simulated material and orchestrating multiple constraints becomes 
challenging \cite{servin2006, nealen2006}. Additionally, the generated penalty forces do not converge in the limit of infinite stiffess, leading
to oscillations unless the timestep is reduced significantly \cite{rubin1957}.

\textcolor{blue}{Maybe explain the challenges with penalty forces a bit better! Also read \cite{terz1987, nealen2006, rubin1957}. 
I just skimmed over \cite{terz1987} for now, but want to make sure that I am citing this correctly. The term penalty forces is not used
in the paper, I am just following the trail from \cite{servin2006}. \cite{nealen2006} is a review that might be intersting to read.
\cite{rubin1957} would be really interesting to read for once, just to understand why strong penalty forces oscillate. Is this a
general problem with penalty forces, or is it an issue with the solver?}

\subsection*{Mass Modification}
The motion of a particle can be influenced by modifying the inverse mass matrix $\bm{M^{-1}}$ of the system. 
For a single particle $\bm{x_i}$, it is $\ddot{\bm{x_i}} = \bm{M_i^{-1} f}$, where $\bm{M_i^{-1}}$ is the inverse mass matrix for 
particle $i$. If, for example, the first diagonal entry of $\bm{M_i^{-1}}$ is zero, no acceleration in the x-direction is possible. 
It is possible to construct modified inverse mass matrices $\bm{W}$ such that the accelerations in the three axes of an arbitraty 
orthogonal coordinate system can be restricted. The modified inverse mass matrices $\bm{W}$ can be used in the LSE that results 
from the implicit integration above. By adding simple velocity and position terms to the system equations the magnitude of the 
change in velocity in each direction and even the exact position of each constrained particle can be controlled. This approach is
called mass modification \cite{baraff1998}. In \cite{baraff1998}, the authors use mass modification to model collision constraints 
between objects and cloth and other user defined constraints. 

Since the velocity and position of each constrained particle is controlled via a single velocity and position term, multiple 
constraints that affect the same particle have to be handled together. This can lead to constraints which affect arbitrarily many
particles. For that reason, self-collisions of cloth are not handled via mass modification in \cite{baraff1998}. Instead, penalty
forces are used. Additionally, accurately constraining particle positions is only possible for particles whose velocity is 
constrained as well. The resulting system is unbanded, but sparse, and is solved using a modified version of the conjugate gradient
(CG) method.

\subsection*{Constraint-based Dynamics}

\subsubsection*{Hard Constraints}
The problem of maintaining hard distance constraints between particles can be formulated as a Differential Algebraic Equation (DAE)
\cite{ascher1995, baraff1996}. In this framework, the standard ODE $\bm{f} = \bm{Ma}$ is handled together with algebraic equations 
that model the constraints on the system. Distance constraints are typically implemented using holonomic constraints of the form 
$\bm{\phi(x)} = 0$. Note that the distance constraint $\bm{\phi}(x)$ is formulated in terms of the particle positions, whereas the ODE
works on accelerations or velocities. Consequently, the constraints need to be differentiated once or twice so that they can be 
combined with the ODE in terms of velocties or accelerations, respectively. \textcolor{blue}{In xPBD, we go the other way! The ODE
is tanslated so that it is in terms of positions, so that it can be handled together with the constraints. Is there a reason nobody bothered to
do this before? What are the challenges here? Is this exactly what xPBD is? Is there a way to view the simplifications made in 
terms of the other frameworks?}. Using $\bm{J} = \frac{\delta \bm{\phi}}{\delta \bm{x}}$, where $\bm{J}$ is a $n_c \times n_d$ matrix
and $n_c$ is the number of scalar constraints, this leads to the following constraint formulations:

\begin{align*}
    \bm{Jv} &= 0 \\
    \bm{Ja} &= \bm{c(v)}
\end{align*}

for some $\bm{c(v)}$. \textcolor{red}{If you check \cite{ascher1995}, see that $c(v)$ also depends on the positions $q$. That should be indicated!} 
Additionally, constraint forces are required in order to link the algebraic constraint equations with the ODE describing the motion of 
the system. It can be shown that the constraint forces $\bm{f_c}$ applied to the particles have to be in the following form in order to 
avoid adding linear and angular momentum to the system \cite{baraff1996}:

\[
    \bm{f_c} = \bm{J^T \lambda}
\]

where the $\lambda$ are the Lagrange multipliers of the constraints. With external forces $\bm{f_e}$, the DAE can now be expressed as follows 
\cite{ascher1995}:

\[
    \begin{pmatrix}
        \bm{M} & \bm{-J^T} \\
        \bm{J} & 0
    \end{pmatrix}
    \begin{pmatrix}
        \bm{a} \\
        \bm{\lambda}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \bm{f_e} \\
        \bm{c(v)}
    \end{pmatrix}
\]

Note that the lower block-row of the system drives towards accelerations that satisfy the constraints imposed by $\bm{\phi(x)}$ (or, striclty 
speaking, the differentiations thereof) exactly. This is indicated by the lower-right zero block in the system matrix in either formulation. 
Thus, the system does not have a solution if constraints are contradictory. \textcolor{blue}{Aren't $\dot{q} = v$ and $\dot{v} = a$ 
also part of the differential equation? Because $c(v)$ and $f_e$ also depend on $q$!}

In \cite{ascher1995}, the DAE is approached by eliminating the $\lambda$ from the system entirely and constructing an ODE in terms of positions
and velocities. In \cite{tournier2015}, the authors suggest applying implicit integration schemes to the system directly by constructing the 
following Karush-Kuhn-Tucker (KKT) equation system:

\[
\begin{pmatrix}
    \bm{M} & \bm{-J^T} \\
    \bm{J} & 0
\end{pmatrix}
\begin{pmatrix}
    \bm{v_{n+1}} \\
    \bm{\mu_{n+1}}
\end{pmatrix}
=
\begin{pmatrix}
    \bm{p} + h\bm{f_e} \\
    0
\end{pmatrix}
\]

Here, the external forces $\bm{f_e}$ and the constraint gradients $\bm{J}$ are considered constant across the timestep and $\bm{J(x_{n+1})}$ 
is not approximated using the Taylor expansion like it is in \cite{baraff1998}. If internal forces are taken into account, the upper-left 
matrix $\bm{M}$ is replaced by the matrix $\bm{H}$ from above.

\textcolor{blue}{Reverse-engineering how the authors arrived at this system is quite enlightening. Start out from the equations of motion 
\cite{ascher1995}
    \begin{align*}
        \dot{\bm{v}} &= \bm{M^{-1}(f - J^T) \lambda}
    \end{align*}
    and perform implicit integration:
    \begin{align*}
        \bm{v_{n+1}} &= \bm{v_n} + h\bm{M^{-1}(f_e(x_{n+1})} - \bm{J^T(x_{n+1})\lambda(x_{n+1}))} \\
        \bm{Mv_{n+1}} &= \bm{p} + h\bm{f_e(x_{n+1})} - h\bm{J^T(x_{n+1})\lambda(x_{n+1})} \\ 
        \bm{Mv_{n+1}} + h\bm{J^T(x_{n+1})\lambda(x_{n+1})} &= \bm{p} + h\bm{f_e(x_{n+1})} \\ 
        \bm{Mv_{n+1}} + \bm{J^T(x_{n+1}) \mu(x_{n+1})} &= \bm{p} + h\bm{f_e(x_{n+1})}
    \end{align*}
}
\textcolor{red}{If we assume that $\bm{f_e}$ and the constraint gradients $\bm{J}$ are constant across the time step, we arrive at the 
formulation from the paper. For the external forces, which are usually only comprised of gravitational forces, this is not a big 
deal. For the constraint gradients, I am not sure what the ramifications are. In \cite{baraff1998}, the Taylor expansion is performed
which requires the compution of second derivatives over the constraint functions. This is not happening here at all! Is this what 
authors mean when they say that the constraints are effectively linearized during one solve, e.g. second page 
of \cite{mueller2020}? Technically, speaking, even if the Taylor expansion is performed, the constraints are linearized, if I 
understand correctly.}

Note that the system matrix is sparse, which can be exploited by sparse-matrix solvers in order to solve the system efficiently
\cite{baraff1996}. Alternatively, the Schur complement can be constructed since the mass matrix in the upper left block is invertible.
This leads to a smaller, albeit less sparse system \cite{tournier2015}:

\[
    \bm{JM^{-1}J^T \mu} = \bm{-JM^{-1}(p + } h \bm{f_e)}
\]

If the constraints are not redundant, $\bm{JM^{-1}J^T}$ is non-singular and symmetric positive definite \cite{baraff1996}, which are desirable
properties for many solvers. According to \cite{servin2006}, the common approaches for linearizing the constraint forces and stabilizing the constraints 
$\bm{\phi(x)} = 0$ are notoriously unstable. Additionally, instabilities in the traverse direction of the constraints occur when the tensile force with 
respect to particle masses is large when using hard constraints \cite{tournier2015}.

\subsubsection*{Compliant Constraints}
By combining ideas from hard constraints and penalty forces, it is possible to formulate the system matrix for hard constraints such that
constraints do not have to be enforced exactly. In this approach, called compliant constraints, the constraints are combined with the the ODE
$\bm{f} = \bm{Ma}$ in a way that allows relaxation of constraints in a physically meaningful manner \cite{servin2006}. The key insight is that
constraints of the form $\bm{\phi(x)}$ are the physical limit of strong potential forces of the form $\frac{k}{2}\bm{\phi^2(x)}$ with high 
stiffness values $k$. However, using large, but finite, stiffness values has adverse affects on the numerical properties of the system matrix. 
Thus, the equations of motion are rewritten in terms of the inverse stiffness. The potential energy for the constraint $\bm{\phi}$ is then 
defined as:

\[
    U(\bm{x}) = \frac{1}{2}\bm{\phi^T(x)} \alpha^{-1}\bm{\phi(x)}
\]

where $\alpha$ is a symmetric, positive definite matrix of dimension $n_c \times n_c$. The correspondence to the penalty terms above is the 
case where $\alpha^{-1}$ is a diagonal matrix with diagonal entries $\frac{1}{k_i}$ for the stiffness $k_i$ of constraint $\bm{\phi(x)}$. The
resulting forces $\bm{f_c} = \delta U / \delta \bm{x} = -J^T\alpha^{-1} \bm{\phi}$. In order to replace the large parameters $\alpha^{-1}$ with
the small $\alpha$ in the equations of motion, artificial variables $\lambda = -\alpha^{-1}\bm{\phi}$ are introduced, yielding 
$\bm{f_c} = J^T\lambda$.

This leads to the following DAE:

\begin{align*}
    \bm{\dot{x}} &= \bm{v} \\
    \bm{M\dot{v}} &= \bm{f_{e} + J^T\lambda} \\
    \alpha\lambda(\bm{x}, t) &= -\bm{\phi}(\bm{x}, t)
\end{align*}

Note, that in the limit of infinite stiffness, the formulation from hard constraints is recovered. By performing backwards differentiation and 
assuming that the constraint gradients are constant across the timestep, it is:

\[
    \bm{\alpha\lambda_{n+1}} = \bm{C}\frac{\bm{\mu_{n+1}}}{h} = -\bm{\phi_{n+1}} \approx -\bm{\phi} - h\bm{Jv_{n+1}}
\]

leading to the following LSE \cite{tournier2015}:

\[
\begin{pmatrix}
    \bm{M} & \bm{-J^T} \\
    \bm{J} & \frac{1}{h^2}\bm{\alpha}
\end{pmatrix}
\begin{pmatrix}
    \bm{v_{n+1}} \\
    \bm{\mu_{n+1}}
\end{pmatrix}
=
\begin{pmatrix}
    \bm{p} + h\bm{f_e} \\
    - \frac{1}{h}\bm{\phi}
\end{pmatrix}
\]

\textcolor{red}{Regarding the backwards differentiation above, this does not perform the Taylor approximation again. It should be something
like:
\begin{align*}
    \phi_+ \approx \phi + h\dot{\phi_+} &= \phi + hJ_+v_+ \\
                                        &\approx \phi + h(J + h\dot{J})v_+ \\
                                        &= \phi + h(J + h\frac{\delta J}{\delta x}\frac{\delta x}{\delta t})v_+ \\
                                        &= \phi + h(J + h\frac{\delta J}{\delta x}v)v_+
\end{align*}
Now, we need second derivatives of the constraints. This can be seen in \cite{baraff1998} and is also mentioned in \cite{servin2006}.
}

This formulation comes with a couple of advantages. Firstly, relaxing the constraints by keeping a finite but large penalty parameter helps
counteracting numerical problems in the presence of over defined or degenerate constraints. Secondly, in comparison to the system from hard
constraints, introducing $\alpha$ in the lower right block of the system matrix makes the system matrix strongly positive definite, which 
is beneficial for many solvers. Lastly, in comparison to penalty forces, entries of large magnitudes in the system matrix due to high 
stiffness terms are exchanged for small entries in terms of inverse stiffness, which improves the condition number of the matrix. 

\textcolor{red}{
    All these concepts from numerics are a bit unclear to me. I might have to go back to some textbook and do some reading to improve my understanding. 
    I might have to go back to some textbook and do some reading to improve my understanding. Not sure the last part is entirely true.}

\textcolor{red}{
    In \cite{servin2006}, a solver based on symplectic Euler which does not require second derivatives is derived. I do not understand some of the
    estimations made in that derivation. In particular the mean of a function $f$ over and interval $(a, b)$ is defined as $\frac{1}{b-a}
    \int_a^b f(x) dx$, so what they are saying does not make a lot of sense.}


\subsection*{Projective Dynamics}

\textcolor{red}{Make sure to change the phrasing. For now, this is mostly copied from the relevant papers \cite{bouaziz2014}.}

\begin{itemize}
    \item Restrict energy potientials to a specific structure. Convex quadratic distance measure from a constraint. The constraints are general
        nonlinear functions that express the desired state of an element.
    \item This structure trades general material nonlinearity (some energy potentials cannot be recreated using with this structure) against 
        efficiency and robustness.
    \item Structure of these constraint-based energy potentials allows efficient local/global optimization (block coordinate descent).
    \item Energy decreases weakly in every iteration without any specific precautions.
    \item For a fixed set of constraints, the linear system of the global step can be pre-factored.
    \item The local steps consists of small independent optimization problems, which can be all executed in parallel.
    \item Robust and efficient, often significantly outperforming classical Newton methods.
    \item Projective dynamics is an extension of \cite{liu2013} from mass-sprint systems to exmploying projection onto constraint sets to 
        simulate general nodal dynamical systems.
    \item Constraint projection is performed globally here, while it is performed locally in (x)PBD
        \begin{itemize}
            \item Understand exactly where and how the projection is achieved in (x)PBD
        \end{itemize}

    \item \textcolor{red}{The global solve (10) has a Hessian matrix according to \cite{bouaziz2014}. Look at the minimization problem with
        fixed projetion variables again }

    \item \cite{bouaziz2014} show some interesting experiments in their results. Additionally, the listed weaknesses of projective dynamics are 
        pretty interesting.
\end{itemize}

% Bibliography ------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{bibliography/references}

\end{document}
